{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f7f6f55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Libraries$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35969ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import load_dataset\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed2fb0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Break$ $Dataset$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c864eb4b",
   "metadata": {},
   "source": [
    "##### *$Break$ is a human annotated dataset of natural language questions and their Question Decomposition Meaning Representations (QDMRs). Break consists of 83,978 examples sampled from 10 question answering datasets over text, images and databases.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b09823",
   "metadata": {},
   "source": [
    "##### Datasets: \n",
    "\n",
    "1. $QDMR$: Contains questions over text, images and databases annotated with their Question Decomposition Meaning Representation. In addition to the train, dev and (hidden) test sets we are provided with lexicon_tokens files. For each question, the lexicon file contains the set of valid tokens that could potentially appear in its decomposition.\n",
    "\n",
    "2. $QDMR high-level$: Contains questions annotated with the high-level variant of QDMR. These decomposition are exclusive to Reading Comprehension tasks. lexicon_tokens files are also provided.\n",
    "\n",
    "3. $Logical-forms$: Contains questions and QDMRs annotated with full logical-forms of QDMR operators + arguments. Full logical-forms were inferred by the annotation-consistency algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2cb739",
   "metadata": {},
   "source": [
    "*$Note$: It is also provided (in the github) the  `app_store_generation.py` file in order to generate valid lexicon tokens for a new example. We need to use the valid_annotation_tokens method. Note that we would still need to format the valid lexicon tokens according to the lexicon file format {\"source\": \"NL question\", \"allowed_tokens\": [valid lexicon tokens]} (for new examples)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd02ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Break$ - $QDMR$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc060f2f",
   "metadata": {},
   "source": [
    "In order to download the dataset we can visit https://github.com/allenai/Break/tree/master/break_dataset/QDMR and: \n",
    "* Option 1: Clone the repo \n",
    "* Option 2: use the load_dataset('break_data', 'QDMR') and if needed download the lexicon token files malually from the github repo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523bed8",
   "metadata": {},
   "source": [
    "The QDMR version contains `train`, `validation` and `test` sets. \n",
    "\n",
    "* Train/Val Set: Provide the questions and the decompositions\n",
    "* Test Set: Provide the questions yet the desomposition is not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c84a9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdmr_dataset = load_dataset('break_data', 'QDMR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf98920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdmr_dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895869a",
   "metadata": {},
   "source": [
    "Each example contains: \n",
    "\n",
    "1. `question_id`: The Break question id, of the format [ORIGINAL DATASET]_[original split]_[original id]. \n",
    "   * e.g., NLVR2_dev_dev-1049-1-1 is from NLVR2 dev split with its NLVR2 id being, dev-1049-1-1.\n",
    "\n",
    "2. `question_text`: Original question text.\n",
    "\n",
    "3. `decomposition`: The annotated QDMR of the question, its steps delimited by ;. \n",
    "   * e.g., return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon.\n",
    "\n",
    "4. `operators`: List of tagged QDMR operators for each step. QDMR operators are fully described in (Section 2) of the paper. The 14 potential operators are, select, project, filter, aggregate, group, superlative, comparative, union, intersection, discard, sort, boolean, arithmetic, comparison. Unidefntified operators are tagged with None.\n",
    "\n",
    "5. `split`: The Break dataset split of the example (train, dev, test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edac7b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': Value(dtype='string', id=None),\n",
       " 'question_text': Value(dtype='string', id=None),\n",
       " 'decomposition': Value(dtype='string', id=None),\n",
       " 'operators': Value(dtype='string', id=None),\n",
       " 'split': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdmr_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6f9b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: ACADEMIC_train_0\n",
      "Split: train\t | Operator: ['select', 'filter']\n",
      "==================================================\n",
      "\n",
      "Question: return me the homepage of PVLDB . \n",
      "\n",
      "Decomposition: return homepages ;return #1 of  PVLDB\n"
     ]
    }
   ],
   "source": [
    "qdmr_example = qdmr_dataset[\"train\"][0]\n",
    "\n",
    "print(f\"ID: {qdmr_example['question_id']}\")\n",
    "print(f\"Split: {qdmr_example['split']}\\t | Operator: {qdmr_example['operators']}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nQuestion: {qdmr_example['question_text']}\")\n",
    "print(f\"\\nDecomposition: {qdmr_example['decomposition']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe05b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Break$ - $QDMR$ $high$ $level$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307dce9",
   "metadata": {},
   "source": [
    "In order to download the dataset we can visit https://github.com/allenai/Break/tree/master/break_dataset/QDMR-high-level and: \n",
    "* Option 1: Clone the repo \n",
    "* Option 2: use the load_dataset('break_data', 'QDMR') and if needed download the lexicon token files malually from the github repo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5418ad",
   "metadata": {},
   "source": [
    "*$Note$: It is also provided (in the github) the  `app_store_generation.py` file in order to generate valid lexicon tokens for a new example. We need to use the valid_annotation_tokens method. Note that we would still need to format the valid lexicon tokens according to the lexicon file format {\"source\": \"NL question\", \"allowed_tokens\": [valid lexicon tokens]} (for new examples)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdc0f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdmr_high_level_dataset = load_dataset('break_data', 'QDMR-high-level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e55b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdmr_high_level_dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105eb37f",
   "metadata": {},
   "source": [
    "Each example contains (same as QDMR): \n",
    "\n",
    "1. `question_id`: The Break question id, of the format [ORIGINAL DATASET]_[original split]_[original id]. \n",
    "   * e.g., NLVR2_dev_dev-1049-1-1 is from NLVR2 dev split with its NLVR2 id being, dev-1049-1-1.\n",
    "\n",
    "2. `question_text`: Original question text.\n",
    "\n",
    "3. `decomposition`: The annotated QDMR of the question, its steps delimited by ;. \n",
    "   * e.g., return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon.\n",
    "\n",
    "4. `operators`: List of tagged QDMR operators for each step. QDMR operators are fully described in (Section 2) of the paper. The 14 potential operators are, select, project, filter, aggregate, group, superlative, comparative, union, intersection, discard, sort, boolean, arithmetic, comparison. Unidefntified operators are tagged with None.\n",
    "\n",
    "5. `split`: The Break dataset split of the example (train, dev, test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20aaeee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: CWQ_train_WebQTest-1_ffaa5abbcbb61976117b13d82a5011ed\n",
      "Split: train\t | Operator: ['select', 'filter']\n",
      "==================================================\n",
      "\n",
      "Question: What office, also held by a member of the Maine House of Representatives, did James K. Polk hold before he was president?\n",
      "\n",
      "Decomposition: return office that was  held by James K. Polk before he was president ;return #1 that was  held by a member of the  Maine House of  Representatives\n"
     ]
    }
   ],
   "source": [
    "qdmr_high_level_example = qdmr_high_level_dataset[\"train\"][0]\n",
    "\n",
    "print(f\"ID: {qdmr_high_level_example['question_id']}\")\n",
    "print(f\"Split: {qdmr_high_level_example['split']}\\t | Operator: {qdmr_high_level_example['operators']}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nQuestion: {qdmr_high_level_example['question_text']}\")\n",
    "print(f\"\\nDecomposition: {qdmr_high_level_example['decomposition']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b6e8c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $QDMR$ $vs.$ $QDMR$ $high$ $level$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43179e3",
   "metadata": {},
   "source": [
    "* In the **QDMR** version, the decomposition is broken down into low-level, programmatic steps. Each step corresponds to a basic logical operation (like *SELECT*, *FILTER*, or *AGGREGATE*), making it ideal for testing a model's ability to generate formal, machine-readable queries.\n",
    "\n",
    "* The **QDMR-high-level** version provides a more abstract and natural decomposition. The steps are sub-questions that are much closer to how a human would reason through the problem, making it a better fit for evaluating the step-by-step thinking of conversational LLMs.\n",
    "\n",
    "For our project, which focuses on evaluating an LLM's ability to perform natural prompt decomposition, the **QDMR-high-level** dataset is the more appropriate choice. Its human-like reasoning style is a better match for the \"step-by-step thinking\" we want to test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1a762",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Dataset$ $Creation$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71bc784",
   "metadata": {},
   "source": [
    "##### $Lexicon$ $Tokens$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c76c95f",
   "metadata": {},
   "source": [
    "1. We visit the https://github.com/allenai/Break/tree/master/break_dataset/QDMR-high-level\n",
    "2. We download the train_lexicon_tokens.json\n",
    "3. We download the dev_lexicon_tokens.json \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe578ab9",
   "metadata": {},
   "source": [
    "*First, we will create a few_shot_examples.json file by taking a sample from the training set. Next, we will build a separate evaluation dataset from the validation set, following the same process. The .json files we are going to create will contain the lexicon tokens provided as well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5227521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function processes a single example from the QDMR high level set.\n",
    "# It extracts the ID, question, decomposition and lexicon tokens. \n",
    "# In the decomposition, the `;` is replaced by /n in order to be more clear for the model.\n",
    "# It returns these elements in a structured dictionary.\n",
    "\n",
    "def decomposition_prompt(qdmr_high_level_example, lexicon_dict) -> dict:\n",
    "    question_id = qdmr_high_level_example['question_id']\n",
    "    question = qdmr_high_level_example['question_text']\n",
    "\n",
    "    # Get the corresponding lexicon tokens for this question\n",
    "    lexicon_tokens = lexicon_dict.get(question, [])\n",
    "\n",
    "    # Clean decomposition\n",
    "    steps = qdmr_high_level_example['decomposition'].split(';')\n",
    "    cleaned_steps = [re.sub(r'\\s+', ' ', step).strip() for step in steps if step.strip()]\n",
    "    decomposition = '\\n'.join(cleaned_steps)\n",
    "\n",
    "    # Final dictionary\n",
    "    dict_prompt = {\n",
    "        \"id\": question_id,\n",
    "        \"question\": question,\n",
    "        \"lexicon tokens\": lexicon_tokens,\n",
    "        \"decomposition\": decomposition\n",
    "    }\n",
    "\n",
    "    return dict_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a7113",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Examples$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d0b5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51d4efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_tokens_filename = '../notebooks/train_lexicon_tokens.json'\n",
    "\n",
    "lexicon_dict = dict()\n",
    "with open(lexicon_tokens_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line.strip())\n",
    "        # Map question text to allowed tokens\n",
    "        lexicon_dict[entry['source']] = entry['allowed_tokens']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccb0e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in qdmr_high_level_dataset[\"train\"]:\n",
    "    if len(few_shot_examples) < 5:\n",
    "        \n",
    "        few_shot_examples.append(decomposition_prompt(example, lexicon_dict))\n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7c3944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "filename = \"qdmr_few_shot.json\"\n",
    "folder = \"../QDMR_dataset/\"\n",
    "\n",
    "# Construct the full path for the file\n",
    "full_path = os.path.join(folder, filename)\n",
    "\n",
    "# Save the file\n",
    "with open(full_path, 'w', encoding='utf-8') as f:\n",
    "\tjson.dump(few_shot_examples, f, ensure_ascii=False, indent=4)\n",
    "print(\"Results have been saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e278b",
   "metadata": {},
   "source": [
    "##### $Evaluation$ $Examples$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18f2b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58d432d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_tokens_filename = '../notebooks/dev_lexicon_tokens.json'\n",
    "\n",
    "lexicon_dict = dict()\n",
    "with open(lexicon_tokens_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line.strip())\n",
    "        # Map question text to allowed tokens\n",
    "        lexicon_dict[entry['source']] = entry['allowed_tokens']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7aae627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in qdmr_high_level_dataset[\"validation\"]:\n",
    "    if len(evaluation) < 5:\n",
    "        \n",
    "        evaluation.append(decomposition_prompt(example, lexicon_dict))\n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56d2b10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'CWQ_dev_WebQTest-1011_c0be4f76a5397ba6d0d06f53905e504b',\n",
       " 'question': 'What Tibetan speaking countries have a population of less than 993885000?',\n",
       " 'lexicon tokens': \"['higher than', 'same as', 'what ', 'and ', 'than ', 'at most', 'distinct', 'two', 'at least', 'or ', 'date', 'on ', '@@14@@', 'countries', 'equal', 'hundred', 'those', 'sorted by', 'elevation', 'which ', '@@6@@', '993885000', 'was ', 'did ', 'population', 'height', 'one', 'that ', 'on', 'did', 'who', 'true', '@@2@@', '100', 'false', 'and', 'was', 'speaking', 'populations', 'who ', 'a ', 'the', 'number of ', '@@16@@', 'if ', 'where', '@@18@@', 'how', 'larger than', 'is ', 'from ', 'a', 'less', 'for each', 'are ', '@@19@@', '@@4@@', '@@11@@', 'distinct ', 'to', 'not ', 'objects', 'with ', ', ', 'lowest', 'in', 'has ', 'zero', 'in ', 'there ', 'lower than', 'highest', '@@9@@', 'than', 'size', 'multiplication', 'with', 'besides ', ',', '@@1@@', 'what', 'have', 'those ', 'of', '@@3@@', 'that', 'there', '@@10@@', '@@5@@', 'both ', '@@15@@', 'number of', 'price', 'any', 'which', 'Tibetan', 'to ', 'how ', 'when ', 'of ', 'division', 'country', 'is', 'sum', 'or', 'if', 'more', '@@12@@', 'smaller than', 'flights', '@@7@@', '@@17@@', 'for each ', 'What', 'speak', 'from', '@@13@@', 'has', 'difference', 'when', 'are', 'any ', '@@8@@', 'both', 'the ', ',  ', 'besides', 'have ', 'where ', 'not']\",\n",
       " 'decomposition': 'return Tibetan speaking countries\\nreturn #1 that have a population of less than 993885000'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "595753b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "filename = \"qdmr_dataset.json\"\n",
    "folder = \"../QDMR_dataset/\"\n",
    "\n",
    "# Construct the full path for the file\n",
    "full_path = os.path.join(folder, filename)\n",
    "\n",
    "# Save the file\n",
    "with open(full_path, 'w', encoding='utf-8') as f:\n",
    "\tjson.dump(evaluation, f, ensure_ascii=False, indent=4)\n",
    "print(\"Results have been saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
