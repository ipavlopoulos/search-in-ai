{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a42135",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Libraries$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "85c1dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4ce8f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $HotpotQA$ - $Distractor$ $version$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e2e77",
   "metadata": {},
   "source": [
    "The distractor version contains a `train` set and a `validation` set. \n",
    "* **Train Set:** Provides the question, the answer, and the two gold standard context paragraphs required to answer it. Here there are no distractors.\n",
    "* **Validation Set:** Contains a question and ~10 paragraphs. The 2 are the gold paragraphs, and 8 are carefully chosen distractors. The model's task is to read these 10 and answer.\n",
    "\n",
    "This version does not contain a `test set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3ee018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will  download the dataset\n",
    "dataset_distractor = load_dataset(\"hotpot_qa\", \"distractor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c78d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_distractor.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd8921",
   "metadata": {},
   "source": [
    "Each example contains:  \n",
    "\n",
    "1. **`id`**: A unique identifier for the question.  \n",
    "2. **`question`**: The question that needs to be answered.  \n",
    "3. **`answer`**: The correct answer to the question.  \n",
    "4. **`type`**: The type of question—either:  \n",
    "   - **`comparison`** (compares two things)  \n",
    "   - **`bridge`** (connects facts to find the answer).  \n",
    "5. **`level`**: How hard the question is (`easy`, `medium`, or `hard`).  \n",
    "6. **`supporting_facts`**: The key facts that prove the answer is correct. Each fact is given as:  \n",
    "   - **`title`**: The title of the paragraph where the fact is found.  \n",
    "   - **`sent_id`**: The sentence number (starting from 0) inside that paragraph.  \n",
    "7. **`context`**: All the paragraphs the model must read to find the answer. Each paragraph has:  \n",
    "   - **`title`**: The paragraph’s title.  \n",
    "   - **`sentences`**: A list of sentences in that paragraph.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac0d4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'answer', 'type', 'level', 'supporting_facts', 'context'],\n",
       "    num_rows: 90447\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_distractor[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26fa8344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 5a7a06935542990198eaf050\n",
      "Level: medium\t | Type: comparison\n",
      "==================================================\n",
      "\n",
      "Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
      "\n",
      "Answer: Arthur's Magazine\n",
      "\n",
      "\n",
      "Supporting Facts\n",
      "--------------------\n",
      "To answer the question, the model needs to find the following information:\n",
      "\n",
      "  - From Article 'Arthur's Magazine':\n",
      "    \"Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.\"\n",
      "\n",
      "  - From Article 'First for Women':\n",
      "    \"First for Women is a woman's magazine published by Bauer Media Group in the USA.\"\n"
     ]
    }
   ],
   "source": [
    "example_distractor = dataset_distractor[\"train\"][0]\n",
    "\n",
    "print(f\"ID: {example_distractor['id']}\")\n",
    "print(f\"Level: {example_distractor['level']}\\t | Type: {example_distractor['type']}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nQuestion: {example_distractor['question']}\")\n",
    "print(f\"\\nAnswer: {example_distractor['answer']}\")\n",
    "\n",
    "print(\"\\n\\nSupporting Facts\")\n",
    "print(\"-\"*20)\n",
    "print(\"To answer the question, the model needs to find the following information:\")\n",
    "\n",
    "# Extract the titles and sentence IDs for the supporting facts\n",
    "supporting_titles = example_distractor['supporting_facts']['title']\n",
    "supporting_sent_ids = example_distractor['supporting_facts']['sent_id']\n",
    "\n",
    "# Get the full context list\n",
    "context_titles = example_distractor['context']['title']\n",
    "context_sentences = example_distractor['context']['sentences']\n",
    "\n",
    "# Loop through the supporting facts and find the full sentences\n",
    "for i in range(len(supporting_titles)):\n",
    "\tfact_title = supporting_titles[i]\n",
    "\tfact_sent_id = supporting_sent_ids[i]\n",
    "\t\n",
    "\t# Find the index of the article in the main context\n",
    "\ttry:\n",
    "\t\tcontext_index = context_titles.index(fact_title)\n",
    "\t\t# Get the actual sentence\n",
    "\t\tsentence = context_sentences[context_index][fact_sent_id]\n",
    "\t\tprint(f\"\\n  - From Article '{fact_title}':\")\n",
    "\t\tprint(f\"    \\\"{sentence}\\\"\")\n",
    "\texcept ValueError:\n",
    "\t\tprint(f\"Error: Could not find the context for title '{fact_title}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c086c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $HotpotQA$ - $Fullwiki$ $version$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a104a64",
   "metadata": {},
   "source": [
    "The fullwiki version contains a `train` set, a `validation` set and a `test` set. \n",
    "* **Train Set:** The train set is the same as in the distractor version. Provides the question, the answer, and the two gold standard context paragraphs required to answer it. Here there are no distractors.\n",
    "* **Validation Set:** The 10 paragraphs in the validation set are not the ground truth context. They are the output of a standard, baseline Information Retrieval (IR) system. The dataset creators ran their own simple search engine over all of Wikipedia to find the 10 most likely paragraphs for each question. \n",
    "* **Test Set:** We are given only the questions. There is no context provided. The system must implement its own retrieval module to search all the articles of the Wikipedia corpus, find the relevant information, and then answer the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "918ea2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fullwiki = load_dataset(\"hotpot_qa\", \"fullwiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e509a56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fullwiki.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d603132",
   "metadata": {},
   "source": [
    "Each example contains the same features as in the distractor version: \n",
    "\n",
    "1. **`id`**: A unique identifier for the question.  \n",
    "2. **`question`**: The question that needs to be answered.  \n",
    "3. **`answer`**: The correct answer to the question.  \n",
    "4. **`type`**: The type of question—either:  \n",
    "   - **`comparison`** (compares two things)  \n",
    "   - **`bridge`** (connects facts to find the answer).  \n",
    "5. **`level`**: How hard the question is (`easy`, `medium`, or `hard`).  \n",
    "6. **`supporting_facts`**: The key facts that prove the answer is correct. Each fact is given as:  \n",
    "   - **`title`**: The title of the paragraph where the fact is found.  \n",
    "   - **`sent_id`**: The sentence number (starting from 0) inside that paragraph.  \n",
    "7. **`context`**: All the paragraphs the model must read to find the answer. Each paragraph has:  \n",
    "   - **`title`**: The paragraph’s title.  \n",
    "   - **`sentences`**: A list of sentences in that paragraph.  \n",
    "\n",
    "$Attention$ : $The$ **$supporting$ _ $facts$** $and$ **$answers$** $are$ $not$ $present$ $in$ $the$ $test$ $set.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dbda7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'answer', 'type', 'level', 'supporting_facts', 'context'],\n",
       "    num_rows: 90447\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fullwiki[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00e353af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 5a7a06935542990198eaf050\n",
      "Level: medium\t | Type: comparison\n",
      "==================================================\n",
      "\n",
      "Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
      "\n",
      "Answer: Arthur's Magazine\n",
      "\n",
      "\n",
      "Supporting Facts\n",
      "--------------------\n",
      "To answer the question, the model needs to find the following information:\n",
      "\n",
      "  - From Article 'Arthur's Magazine':\n",
      "    \"Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.\"\n",
      "\n",
      "  - From Article 'First for Women':\n",
      "    \"First for Women is a woman's magazine published by Bauer Media Group in the USA.\"\n"
     ]
    }
   ],
   "source": [
    "example_fullwiki = dataset_fullwiki[\"train\"][0]\n",
    "\n",
    "print(f\"ID: {example_fullwiki['id']}\")\n",
    "print(f\"Level: {example_fullwiki['level']}\\t | Type: {example_fullwiki['type']}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nQuestion: {example_fullwiki['question']}\")\n",
    "print(f\"\\nAnswer: {example_fullwiki['answer']}\")\n",
    "\n",
    "print(\"\\n\\nSupporting Facts\")\n",
    "print(\"-\"*20)\n",
    "print(\"To answer the question, the model needs to find the following information:\")\n",
    "\n",
    "# Extract the titles and sentence IDs for the supporting facts\n",
    "supporting_titles = example_fullwiki['supporting_facts']['title']\n",
    "supporting_sent_ids = example_fullwiki['supporting_facts']['sent_id']\n",
    "\n",
    "# Get the full context list\n",
    "context_titles = example_fullwiki['context']['title']\n",
    "context_sentences = example_fullwiki['context']['sentences']\n",
    "\n",
    "# Loop through the supporting facts and find the full sentences\n",
    "for i in range(len(supporting_titles)):\n",
    "\tfact_title = supporting_titles[i]\n",
    "\tfact_sent_id = supporting_sent_ids[i]\n",
    "\t\n",
    "\t# Find the index of the article in the main context\n",
    "\ttry:\n",
    "\t\tcontext_index = context_titles.index(fact_title)\n",
    "\t\t# Get the actual sentence\n",
    "\t\tsentence = context_sentences[context_index][fact_sent_id]\n",
    "\t\tprint(f\"\\n  - From Article '{fact_title}':\")\n",
    "\t\tprint(f\"    \\\"{sentence}\\\"\")\n",
    "\texcept ValueError:\n",
    "\t\tprint(f\"Error: Could not find the context for title '{fact_title}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2551076",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Distractor$ $vs.$ $Fullwiki$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2df9b9",
   "metadata": {},
   "source": [
    "* In the **distractor** setting, the 10 paragraphs are a \"hand-picked\" reasoning challenge where the 2 gold paragraphs are guaranteed to be present.\n",
    "\n",
    "* In the **fullwiki** validation setting, the 10 paragraphs are the pre-computed results of a simple search engine. They are provided for convenience during development and to create a fair test of reading comprehension on a *retrieved* set of documents, where the necessary information is not guaranteed to be present.\n",
    "\n",
    "\n",
    "\n",
    "For our project, which focuses on decomposition (a reasoning task), sticking with the **distractor** data seems to be a valid approach, since we are primarily testing the model's ability to reason over a given context, not its ability to retrieve documents from all of Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47285582",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Dataset$ $Creation$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246334d1",
   "metadata": {},
   "source": [
    "*First, we will create a few_shot_examples.json file by manually decomposing questions from the training set, using their supporting sentences as a guide. Next, we will build a separate evaluation dataset from the validation set, following the same manual decomposition process.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556bebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function processes a single example from the HotpotQA 'distractor' set.\n",
    "# It extracts the ID, question, answer, and reconstructs the list of \n",
    "# supporting sentences needed to arrive at the correct answer.\n",
    "# It returns these elements in a structured dictionary.\n",
    "\n",
    "def decomposition_prompt(example_distractor):\n",
    "\tid = example_distractor['id']\n",
    "\tquestion = example_distractor['question']\n",
    "\tanswer = example_distractor['answer']\n",
    "\tsupporting_sentences = []\n",
    "\tfor title, sent_id in zip(example_distractor[\"supporting_facts\"][\"title\"], example_distractor[\"supporting_facts\"][\"sent_id\"]):\n",
    "\t\tfor context_title, sentences in zip(example_distractor[\"context\"][\"title\"], example_distractor[\"context\"][\"sentences\"]):\n",
    "\t\t\tif title == context_title: \n",
    "\t\t\t\tsupporting_sentences.append(sentences[sent_id].strip())\n",
    "\n",
    "\tsupporting_sentences.append(f\"Aggregating the above we conclude that the answer is: {answer}\")\n",
    "\n",
    "\tdict_prompt = {\"id\":id, \"question\":question, \"supporting_sentences\":supporting_sentences, \"answer\":answer}\n",
    "\t\n",
    "\treturn dict_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb78698d",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Examples$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e818dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0719569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in dataset_distractor[\"train\"]:\n",
    "    if len(few_shot_examples) < 5:\n",
    "        few_shot_examples.append(decomposition_prompt(example))\n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "82ae6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples[0][\"decomposition\"] = [\n",
    "    \t\t\t\"When was Arthur's Magazine started?\",\n",
    "                \"When was First for Women magazine started?\",\n",
    "                \"Which of the two starting dates is earlier?\"]\n",
    "\n",
    "few_shot_examples[1][\"decomposition\"] = [\n",
    "                \"Which hotel company is the Oberoi family associated with?\",\n",
    "                \"Where is the head office of The Oberoi Group located?\"]\n",
    "\n",
    "few_shot_examples[2][\"decomposition\"] = [\n",
    "    \t\t\t\"Who created the character Milhouse in 'The Simpsons'?\",\n",
    "                \"After whom did Matt Groening name the character Milhouse?\"]\n",
    "\n",
    "few_shot_examples[3][\"decomposition\"] = [\n",
    "\t\t\t\t\"Who was James Henry Miller?\",\n",
    "                \"Who was James Henry Miller's wife?\",\n",
    "                \"What was James Henry Miller's wife nationality?\"]\n",
    "\n",
    "few_shot_examples[4][\"decomposition\"] = [\n",
    "                \"What chemical is Cadmium Chloride slightly soluble in?\",\n",
    "                \"What is another name for this chemical?\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4785702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "filename = \"hotpot_few_shot.json\"\n",
    "folder = \"../HotpotQA_dataset/\"\n",
    "\n",
    "# Construct the full path for the file\n",
    "full_path = os.path.join(folder, filename)\n",
    "\n",
    "# Save the file\n",
    "with open(full_path, 'w', encoding='utf-8') as f:\n",
    "\tjson.dump(few_shot_examples, f, ensure_ascii=False, indent=4)\n",
    "print(\"Results have been saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8698724",
   "metadata": {},
   "source": [
    "##### $Evaluation$ $Examples$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dbf63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in dataset_distractor[\"validation\"]:\n",
    "    if len(evaluation) < 5:\n",
    "        evaluation.append(decomposition_prompt(example))\n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a91fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation[0][\"decomposition\"] = [\n",
    "    \t\t\t\"What nationality Scott Derrickson had?\",\n",
    "                \"What nationality Ed Wood had?\",\n",
    "                \"Was the nationality the same?\"]\n",
    "\n",
    "evaluation[1][\"decomposition\"] = [\n",
    "                \"Who potrayed Caroliss Archer in the film Kiss and Tell?\",\n",
    "                \"What governement position was held by her?\"]\n",
    "\n",
    "evaluation[2][\"decomposition\"] = [\n",
    "    \t\t\t\"What book series has companion books, which narrate the story of an enslaved alien species?\",\n",
    "                \"Is this book series classified as science fantasy and written for young adults?\",\n",
    "                \"Is this series narrated in the first person?\"]\n",
    "\n",
    "evaluation[3][\"decomposition\"] = [\n",
    "\t\t\t\t\"What is Laleli Mosque?\",\n",
    "                \"Where is Laleli Mosque located?\"\n",
    "                \"What is Esma Sultan Mansion?\",\n",
    "                \"Where is Esma Sultan Mansion located?\"\n",
    "                \"Are they located in the same neighborhood?\"]\n",
    "\n",
    "evaluation[4][\"decomposition\"] = [\n",
    "                \"Who directed the the romantic comedy 'Big Stone Gap'?\",\n",
    "                \"In what New York city is the director located?\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42ed39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "filename = \"hotpot_dataset.json\"\n",
    "folder = \"../HotpotQA_dataset/\"\n",
    "\n",
    "# Construct the full path for the file\n",
    "full_path = os.path.join(folder, filename)\n",
    "\n",
    "# Save the file\n",
    "with open(full_path, 'w', encoding='utf-8') as f:\n",
    "\tjson.dump(evaluation, f, ensure_ascii=False, indent=4)\n",
    "print(\"Results have been saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
