{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c55d342",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Purpose$ $of$ $the$ $Notebook$ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd458f",
   "metadata": {},
   "source": [
    "The goal of this notebook is to generate predictions on several datasets:  \n",
    "\n",
    "- **QDMR**  \n",
    "- **HotpotQA**  \n",
    "- **StrategyQA**  \n",
    "\n",
    "\n",
    "#### $Predictions$  \n",
    "- Predictions were generated for **5 examples**.  \n",
    "- Shot examples: **3**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fbb911",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Libraries$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa51d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, AutoModel\n",
    "from huggingface_hub import notebook_login\n",
    "import textwrap\n",
    "import sys\n",
    "utils_path = os.path.abspath(\"../utils\")\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "import re\n",
    "from run_experiment import run_experiment\n",
    "from manage_folders import save_results_to_json, dataset_folders\n",
    "from retriever import DynamicRetriever\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4968f51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Model$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebaac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model name \n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b47795a",
   "metadata": {},
   "source": [
    "##### $Bnb$ $Configuration$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ece24",
   "metadata": {},
   "source": [
    "\n",
    "The `bnb_config` creates a configuration to shrink the language model. \n",
    "\n",
    "* *Compresses the Model:* It tells the system to load the model in a \"compressed\" 4-bit format instead of its full 16-bit size.\n",
    "* *Saves Memory:* This makes the model about 4 times smaller, allowing it to run on computers with less memory (RAM and VRAM).\n",
    "* *Maintains Performance:* It uses clever tricks (like doing the actual math in 16-bit) to ensure the shrunken model is still fast and accurate.\n",
    "\n",
    "Essentially, it's a set of instructions to make a huge model fit on the computer with minimal loss in quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5617323",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f55ab5",
   "metadata": {},
   "source": [
    "##### $Tokenizer$ $Set-up$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6730207c",
   "metadata": {},
   "source": [
    "This code sets up a `tokenizer` for the language model.\n",
    "It loads a pre-trained tokenizer, makes sure there's a padding token (for making all inputs the same length), and if no chat template is found, it manually adds one specifically for Llama 3.x models to format conversations correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c568dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer. The library will handle the chat template automatically.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"pad_token not set. Setting it to eos_token for this model.\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer for {model_name} loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741221c3",
   "metadata": {},
   "source": [
    "#### $System$ $Prompt$\n",
    "\n",
    "To ensure the model performs the intended taskâ€”such as question decompositionâ€”we provide clear and detailed instructions. This involves defining the modelâ€™s role, outlining the steps it should follow, and specifying the desired structure of the output. By being as explicit as possible, we guide the model toward producing consistent and accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an expert at breaking down complex questions into a numbered sequence of simple, factual sub-questions. \"\n",
    "    \"After listing all sub-questions DO NOT provide anything else.\\n\\n\"\n",
    "    \n",
    "    \"Follow this exact format:\\n\"\n",
    "    \"subq1: <sub-question>\\n\"\n",
    "    \"subq2: <sub-question>\\n\"\n",
    "    \"... continue numbering sequentially as needed ...\\n\"\n",
    "    \n",
    "    \"Do NOT repeat the original question. \"\n",
    "    \"Do NOT include commentary, explanations, assumptions, or any extra text. \"\n",
    "    \"Strictly adhere to the numbering and format above.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0dd4c3",
   "metadata": {},
   "source": [
    "#### $Parameters$ $for$ $the$ $Model$ $Configuration:$\n",
    "\n",
    "* **`max_new_tokens`**\n",
    "  Defines the maximum number of tokens the model can generate in a single response.\n",
    "\n",
    "  * Example: `max_new_tokens=512` means the model will not produce more than 512 tokens beyond the prompt, preventing it from generating endlessly.\n",
    "\n",
    "* **`do_sample`**\n",
    "  Controls whether the model generates text deterministically or stochastically.\n",
    "\n",
    "  * **`do_sample=False`** â†’ Deterministic: the model always picks the most likely next token, producing the same output for the same input. Useful for comparison and evaluation.\n",
    "  * **`do_sample=True`** â†’ Stochastic: the model samples from the probability distribution, allowing for more variety and creativity. Choices are influenced by parameters such as *temperature* and *top_p*.\n",
    "\n",
    "  ðŸ”¹ For **TinyLlama**, we use:\n",
    "\n",
    "  * `do_sample=True`\n",
    "  * `temperature=0.7`\n",
    "  * `top_p=0.9`\n",
    "\n",
    "<!-- * **`repetition_penalty`**\n",
    "  Adjusts the likelihood of repeating tokens to encourage more diverse output.\n",
    "\n",
    "  * Default: `1.0` (no penalty)\n",
    "  * Typical range: `1.1 â€“ 2.0`\n",
    "\n",
    "    * ~1.2 â†’ light penalty\n",
    "    * â‰¥1.5 â†’ strong penalty -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673107a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model specific configuration\n",
    "model_config = {\n",
    "    \"model_id\": model_name,\n",
    "    \"uses_system_prompt\": True,  # TinyLlama supports a system prompt\n",
    "    \"system_prompt\": system_prompt,\n",
    "    \"generation_params\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "\t\t\"top_p\": 0.9,\n",
    "        # \"repetition_penalty\": 1.2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the model for our file names\n",
    "model_file_name = \"TinyLlama-1.1B_results.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe4b5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $QDMR$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086eb8d6",
   "metadata": {},
   "source": [
    "We use a helper function `dataset_folders(base_results_folder, dataset_path, few_shot_examples_path)` that streamlines the setup process for running experiments on a new dataset. It handles three key tasks:\n",
    "\n",
    "1.  **Creates Output Directories:** It takes a base folder path and automatically creates the `zero_shot` and `few_shot` subdirectories where the model's predictions will be saved.\n",
    "2.  **Loads Evaluation Data:** It reads the main dataset file (e.g., `hotpot_dataset.json`) containing the questions to be evaluated.\n",
    "3.  **Loads Few-Shot Examples:** It reads the corresponding file containing the high-quality examples for few-shot prompting.\n",
    "\n",
    "The function returns a single, convenient dictionary containing all these assets (the loaded data and the output folder paths), which can then be easily used by the main experiment functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f3f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdmr_base_folder = \"../QDMR/llm_predictions/static\"\n",
    "qdmr_dataset_file = \"../QDMR/QDMR_examples/qdmr_evaluation.json\" # Define the path to the QDMR dataset\n",
    "qdmr_fewshot_file = \"../QDMR/QDMR_examples/qdmr_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(qdmr_base_folder, qdmr_dataset_file, qdmr_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "qdmr_data_full = data_assets[\"data_full\"]\n",
    "qdmr_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22046038",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f235727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "\n",
    "zero_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config,  \n",
    "    num_shots=0,\n",
    "    batch_size=5\n",
    ")\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9826173",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Static$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"static\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc15f2b8",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Random$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdmr_base_folder = \"../QDMR/llm_predictions/random\"\n",
    "qdmr_dataset_file = \"../QDMR/QDMR_examples/qdmr_evaluation.json\" # Define the path to the QDMR dataset\n",
    "qdmr_fewshot_file = \"../QDMR/QDMR_examples/qdmr_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(qdmr_base_folder, qdmr_dataset_file, qdmr_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "qdmr_data_full = data_assets[\"data_full\"]\n",
    "qdmr_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2295797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Random-3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"random\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "\tbatch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0fc7a",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Dynamic$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559aa97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdmr_base_folder = \"../QDMR/llm_predictions/dynamic\"\n",
    "qdmr_dataset_file = \"../QDMR/QDMR_examples/qdmr_evaluation.json\" # Define the path to the QDMR dataset\n",
    "qdmr_fewshot_file = \"../QDMR/QDMR_examples/qdmr_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(qdmr_base_folder, qdmr_dataset_file, qdmr_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "qdmr_data_full = data_assets[\"data_full\"]\n",
    "qdmr_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6813fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Dynamic-3-Shot Experiment\")\n",
    "\n",
    "retriever_instance = DynamicRetriever(shot_examples)\n",
    "\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"dynamic\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=retriever_instance,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a335e8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $HotpotQA$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the dataset we want to use\n",
    "hotpot_base_folder = '../HotpotQA/llm_predictions/static/'\n",
    "hotpot_dataset_file = '../HotpotQA/HotpotQA_examples/hotpot_evaluation.json'\n",
    "hotpot_fewshot_file = '../HotpotQA/HotpotQA_examples/hotpot_few_shot.json'\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(hotpot_base_folder, hotpot_dataset_file, hotpot_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "hotpot_data_full = data_assets[\"data_full\"]\n",
    "hotpot_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a9a47",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "\n",
    "zero_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config,  \n",
    "    num_shots=0,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5143bb",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Static$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"static\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d5db6",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Random$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f312a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the dataset we want to use\n",
    "hotpot_base_folder = '../HotpotQA/llm_predictions/random/'\n",
    "hotpot_dataset_file = '../HotpotQA/HotpotQA_examples/hotpot_evaluation.json'\n",
    "hotpot_fewshot_file = '../HotpotQA/HotpotQA_examples/hotpot_few_shot.json'\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(hotpot_base_folder, hotpot_dataset_file, hotpot_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "hotpot_data_full = data_assets[\"data_full\"]\n",
    "hotpot_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911279f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Random-3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"random\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa53aef",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Dynamic$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the dataset we want to use\n",
    "hotpot_base_folder = '../HotpotQA/llm_predictions/dynamic/'\n",
    "hotpot_dataset_file = '../HotpotQA/HotpotQA_examples/hotpot_evaluation.json'\n",
    "hotpot_fewshot_file = '../HotpotQA/HotpotQA_examples/hotpot_few_shot.json'\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(hotpot_base_folder, hotpot_dataset_file, hotpot_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "hotpot_data_full = data_assets[\"data_full\"]\n",
    "hotpot_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Dynamic-3-Shot Experiment\")\n",
    "\n",
    "retriever_instance = DynamicRetriever(shot_examples)\n",
    "\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"dynamic\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=retriever_instance,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "\tbatch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee4911",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $StrategyQA$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7004ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "strategyqa_base_folder = '../StrategyQA/llm_predictions/static'\n",
    "strategyqa_dataset_file = \"../StrategyQA/StrategyQA_examples/strategyqa_evaluation.json\" # Define the path to the strategyqa dataset\n",
    "strategyqa_fewshot_file = \"../StrategyQA/StrategyQA_examples/strategyqa_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(strategyqa_base_folder, strategyqa_dataset_file, strategyqa_fewshot_file,  few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "strategyqa_data_full = data_assets[\"data_full\"]\n",
    "strategyqa_data = data_assets[\"data_subset\"] # This is the subset of 5 examples\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bee393",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25cbc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "\n",
    "zero_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config,  \n",
    "    num_shots=0,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debd9407",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Static$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"static\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eeada7",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Random$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "strategyqa_base_folder = '../StrategyQA/llm_predictions/random'\n",
    "strategyqa_dataset_file = \"../StrategyQA/StrategyQA_examples/strategyqa_evaluation.json\" # Define the path to the strategyqa dataset\n",
    "strategyqa_fewshot_file = \"../StrategyQA/StrategyQA_examples/strategyqa_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(strategyqa_base_folder, strategyqa_dataset_file, strategyqa_fewshot_file,  few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "strategyqa_data_full = data_assets[\"data_full\"]\n",
    "strategyqa_data = data_assets[\"data_subset\"] # This is the subset of 5 examples\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Random-3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"random\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb823a19",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Dynamic$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86498ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "strategyqa_base_folder = '../StrategyQA/llm_predictions/dynamic'\n",
    "strategyqa_dataset_file = \"../StrategyQA/StrategyQA_examples/strategyqa_evaluation.json\" # Define the path to the strategyqa dataset\n",
    "strategyqa_fewshot_file = \"../StrategyQA/StrategyQA_examples/strategyqa_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(strategyqa_base_folder, strategyqa_dataset_file, strategyqa_fewshot_file,  few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "strategyqa_data_full = data_assets[\"data_full\"]\n",
    "strategyqa_data = data_assets[\"data_subset\"] # This is the subset of 5 examples\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Dynamic-3-Shot Experiment\")\n",
    "\n",
    "retriever_instance = DynamicRetriever(shot_examples)\n",
    "\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"dynamic\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=retriever_instance,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
