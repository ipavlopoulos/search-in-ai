{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ff2957",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Purpose$ $of$ $the$ $Notebook$ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d843a",
   "metadata": {},
   "source": [
    "The goal of this notebook is to generate predictions on several datasets:  \n",
    "\n",
    "- **QDMR**  \n",
    "- **HotpotQA**  \n",
    "- **StrategyQA**  \n",
    "\n",
    "\n",
    "#### $Predictions$  \n",
    "- Predictions were generated for **5 examples**.  \n",
    "- Shot examples: **3**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199d72a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Libraries$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfcc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, AutoModel\n",
    "from huggingface_hub import notebook_login\n",
    "import textwrap\n",
    "import sys\n",
    "utils_path = os.path.abspath(\"../utils\")\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "import re\n",
    "from run_experiment import run_experiment\n",
    "from manage_folders import save_results_to_json, dataset_folders\n",
    "from retriever import DynamicRetriever\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af14ff3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Model$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a541ce3f",
   "metadata": {},
   "source": [
    "##### $Model$ $Access$\n",
    "\n",
    "In order to access to the model we need to:\n",
    "1. Visit the webpage of the model: https://huggingface.co/google/gemma-7b-it\n",
    "2. Log in to our HF account\n",
    "3. Click on `Terms` \n",
    "4. Write yout contact information (email and name)\n",
    "5. Accept the terms\n",
    "6. Run `from huggingface_hub import notebook_login` and `notebook_login()` \n",
    "7. Create a new token with `read` rights\n",
    "8. Copy the token and paste it in the notebook and press `Enter`\n",
    "\n",
    "\n",
    "\n",
    "*It might take some minutes*...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bfe71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d90e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model name\n",
    "model_name = \"google/gemma-7b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae37ae",
   "metadata": {},
   "source": [
    "##### $Bnb$ $Configuration$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78f15b",
   "metadata": {},
   "source": [
    "\n",
    "The `bnb_config` creates a configuration to shrink the language model. \n",
    "\n",
    "* *Compresses the Model:* It tells the system to load the model in a \"compressed\" 4-bit format instead of its full 16-bit size.\n",
    "* *Saves Memory:* This makes the model about 4 times smaller, allowing it to run on computers with less memory (RAM and VRAM).\n",
    "* *Maintains Performance:* It uses clever tricks (like doing the actual math in 16-bit) to ensure the shrunken model is still fast and accurate.\n",
    "\n",
    "Essentially, it's a set of instructions to make a huge model fit on the computer with minimal loss in quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e410cd",
   "metadata": {},
   "source": [
    "##### $Tokenizer$ $Set-up$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff856b3",
   "metadata": {},
   "source": [
    "This code sets up a `tokenizer` for the language model.\n",
    "It loads a pre-trained tokenizer, makes sure there's a padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23fead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer. The library will handle the chat template automatically.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"pad_token not set. Setting it to eos_token for this model.\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer for {model_name} loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aed9d5",
   "metadata": {},
   "source": [
    "#### $System$ $Prompt$\n",
    "\n",
    "To ensure the model performs the intended taskâ€”such as question decompositionâ€”we provide clear and detailed instructions. This involves defining the modelâ€™s role, outlining the steps it should follow, and specifying the desired structure of the output. By being as explicit as possible, we guide the model toward producing consistent and accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an expert at breaking down complex questions into a numbered sequence of simple, factual sub-questions. \"\n",
    "    \"After listing all sub-questions DO NOT provide anything else.\\n\\n\"\n",
    "    \n",
    "    \"Follow this exact format:\\n\"\n",
    "    \"subq1: <sub-question>\\n\"\n",
    "    \"subq2: <sub-question>\\n\"\n",
    "    \"... continue numbering sequentially as needed ...\\n\"\n",
    "    \n",
    "    \"Do NOT repeat the original question. \"\n",
    "    \"Do NOT include commentary, explanations, assumptions, or any extra text. \"\n",
    "    \"Strictly adhere to the numbering and format above.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d4b2b7",
   "metadata": {},
   "source": [
    "#### $Parameters$ $for$ $the$ $Model$ $Configuration:$\n",
    "\n",
    "* **`max_new_tokens`**\n",
    "  Defines the maximum number of tokens the model can generate in a single response.\n",
    "\n",
    "  * Example: `max_new_tokens=512` means the model will not produce more than 512 tokens beyond the prompt, preventing it from generating endlessly.\n",
    "\n",
    "* **`do_sample`**\n",
    "  Controls whether the model generates text deterministically or stochastically.\n",
    "\n",
    "  * **`do_sample=False`** â†’ Deterministic: the model always picks the most likely next token, producing the same output for the same input. Useful for comparison and evaluation.\n",
    "  * **`do_sample=True`** â†’ Stochastic: the model samples from the probability distribution, allowing for more variety and creativity. Choices are influenced by parameters such as *temperature* and *top_p*.\n",
    "\n",
    "  ðŸ”¹ For **Gemma-7b-it**, we use:\n",
    "\n",
    "  * `do_sample=True`\n",
    "  * `temperature=0.7`\n",
    "  * `top_p=0.9`\n",
    "\n",
    "<!-- * **`repetition_penalty`**\n",
    "  Adjusts the likelihood of repeating tokens to encourage more diverse output.\n",
    "\n",
    "  * Default: `1.0` (no penalty)\n",
    "  * Typical range: `1.1 â€“ 2.0`\n",
    "\n",
    "    * ~1.2 â†’ light penalty\n",
    "    * â‰¥1.5 â†’ strong penalty -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model specific configuration\n",
    "model_config = {\n",
    "    \"model_id\": model_name,\n",
    "    \"uses_system_prompt\": False,  # Gemma doesnt support a system prompt\n",
    "    \"system_prompt\": system_prompt,\n",
    "    \"generation_params\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "\t\t\"top_p\": 0.9,\n",
    "        # \"repetition_penalty\": 1.2\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0023902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the model for our file names\n",
    "model_file_name = \"Gemma-7b-it_results.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147e4d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $QDMR$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8361cacb",
   "metadata": {},
   "source": [
    "We use a helper function `dataset_folders(base_results_folder, dataset_path, few_shot_examples_path)` that streamlines the setup process for running experiments on a new dataset. It handles three key tasks:\n",
    "\n",
    "1.  **Creates Output Directories:** It takes a base folder path and automatically creates the `zero_shot` and `few_shot` subdirectories where the model's predictions will be saved.\n",
    "2.  **Loads Evaluation Data:** It reads the main dataset file (e.g., `hotpot_dataset.json`) containing the questions to be evaluated.\n",
    "3.  **Loads Few-Shot Examples:** It reads the corresponding file containing the high-quality examples for few-shot prompting.\n",
    "\n",
    "The function returns a single, convenient dictionary containing all these assets (the loaded data and the output folder paths), which can then be easily used by the main experiment functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdmr_base_folder = \"../QDMR/llm_predictions/static\"\n",
    "qdmr_dataset_file = \"../QDMR/QDMR_examples/qdmr_evaluation.json\" # Define the path to the QDMR dataset\n",
    "qdmr_fewshot_file = \"../QDMR/QDMR_examples/qdmr_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(qdmr_base_folder, qdmr_dataset_file, qdmr_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "qdmr_data_full = data_assets[\"data_full\"]\n",
    "qdmr_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54534315",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81596761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "\n",
    "zero_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config,  \n",
    "    num_shots=0,\n",
    "    batch_size=5\n",
    ")\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689cbf2",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Static$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e043bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"static\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe03d1c",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Random$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7619ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdmr_base_folder = \"../QDMR/llm_predictions/random\"\n",
    "qdmr_dataset_file = \"../QDMR/QDMR_examples/qdmr_evaluation.json\" # Define the path to the QDMR dataset\n",
    "qdmr_fewshot_file = \"../QDMR/QDMR_examples/qdmr_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(qdmr_base_folder, qdmr_dataset_file, qdmr_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "qdmr_data_full = data_assets[\"data_full\"]\n",
    "qdmr_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f385971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Random-3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"random\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "\tbatch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94741bd4",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Dynamic$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b188911",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdmr_base_folder = \"../QDMR/llm_predictions/dynamic\"\n",
    "qdmr_dataset_file = \"../QDMR/QDMR_examples/qdmr_evaluation.json\" # Define the path to the QDMR dataset\n",
    "qdmr_fewshot_file = \"../QDMR/QDMR_examples/qdmr_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(qdmr_base_folder, qdmr_dataset_file, qdmr_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "qdmr_data_full = data_assets[\"data_full\"]\n",
    "qdmr_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Dynamic-3-Shot Experiment\")\n",
    "\n",
    "retriever_instance = DynamicRetriever(shot_examples)\n",
    "\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"dynamic\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=retriever_instance,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8108fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $HotpotQA$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233e41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the dataset we want to use\n",
    "hotpot_base_folder = '../HotpotQA/llm_predictions/static/'\n",
    "hotpot_dataset_file = '../HotpotQA/HotpotQA_examples/hotpot_evaluation.json'\n",
    "hotpot_fewshot_file = '../HotpotQA/HotpotQA_examples/hotpot_few_shot.json'\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(hotpot_base_folder, hotpot_dataset_file, hotpot_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "hotpot_data_full = data_assets[\"data_full\"]\n",
    "hotpot_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d60860",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "\n",
    "zero_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config,  \n",
    "    num_shots=0,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd137de6",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Static$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"static\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78829156",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Random$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b87cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the dataset we want to use\n",
    "hotpot_base_folder = '../HotpotQA/llm_predictions/random/'\n",
    "hotpot_dataset_file = '../HotpotQA/HotpotQA_examples/hotpot_evaluation.json'\n",
    "hotpot_fewshot_file = '../HotpotQA/HotpotQA_examples/hotpot_few_shot.json'\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(hotpot_base_folder, hotpot_dataset_file, hotpot_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "hotpot_data_full = data_assets[\"data_full\"]\n",
    "hotpot_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Random-3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"random\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc405c",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Dynamic$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the dataset we want to use\n",
    "hotpot_base_folder = '../HotpotQA/llm_predictions/dynamic/'\n",
    "hotpot_dataset_file = '../HotpotQA/HotpotQA_examples/hotpot_evaluation.json'\n",
    "hotpot_fewshot_file = '../HotpotQA/HotpotQA_examples/hotpot_few_shot.json'\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(hotpot_base_folder, hotpot_dataset_file, hotpot_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "hotpot_data_full = data_assets[\"data_full\"]\n",
    "hotpot_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Dynamic-3-Shot Experiment\")\n",
    "\n",
    "retriever_instance = DynamicRetriever(shot_examples)\n",
    "\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"dynamic\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=retriever_instance,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "\tbatch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a0e7de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $StrategyQA$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d46b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "strategyqa_base_folder = '../StrategyQA/llm_predictions/static'\n",
    "strategyqa_dataset_file = \"../StrategyQA/StrategyQA_examples/strategyqa_evaluation.json\" # Define the path to the strategyqa dataset\n",
    "strategyqa_fewshot_file = \"../StrategyQA/StrategyQA_examples/strategyqa_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(strategyqa_base_folder, strategyqa_dataset_file, strategyqa_fewshot_file,  few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "strategyqa_data_full = data_assets[\"data_full\"]\n",
    "strategyqa_data = data_assets[\"data_subset\"] # This is the subset of 5 examples\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b706e4",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e73777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "\n",
    "zero_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config,  \n",
    "    num_shots=0,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b225a",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Static$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"static\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfac1a31",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Random$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "strategyqa_base_folder = '../StrategyQA/llm_predictions/random'\n",
    "strategyqa_dataset_file = \"../StrategyQA/StrategyQA_examples/strategyqa_evaluation.json\" # Define the path to the strategyqa dataset\n",
    "strategyqa_fewshot_file = \"../StrategyQA/StrategyQA_examples/strategyqa_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(strategyqa_base_folder, strategyqa_dataset_file, strategyqa_fewshot_file,  few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "strategyqa_data_full = data_assets[\"data_full\"]\n",
    "strategyqa_data = data_assets[\"data_subset\"] # This is the subset of 5 examples\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44161ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Random-3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"random\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e7ef94",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Dynamic$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2638794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "strategyqa_base_folder = '../StrategyQA/llm_predictions/dynamic'\n",
    "strategyqa_dataset_file = \"../StrategyQA/StrategyQA_examples/strategyqa_evaluation.json\" # Define the path to the strategyqa dataset\n",
    "strategyqa_fewshot_file = \"../StrategyQA/StrategyQA_examples/strategyqa_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(strategyqa_base_folder, strategyqa_dataset_file, strategyqa_fewshot_file,  few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "strategyqa_data_full = data_assets[\"data_full\"]\n",
    "strategyqa_data = data_assets[\"data_subset\"] # This is the subset of 5 examples\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb18f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Dynamic-3-Shot Experiment\")\n",
    "\n",
    "retriever_instance = DynamicRetriever(shot_examples)\n",
    "\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"dynamic\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=retriever_instance,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
