{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Purpose$ $of$ $the$ $Notebook$ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ace3b5",
   "metadata": {},
   "source": [
    "The goal of this notebook is to generate predictions on several datasets:  \n",
    "\n",
    "- **QDMR**  \n",
    "- **HotpotQA**  \n",
    "- **StrategyQA**  \n",
    "\n",
    "\n",
    "#### $Predictions$  \n",
    "- Predictions were generated for **5 examples**.  \n",
    "- Shot examples: **3**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab189d4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Libraries$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, AutoModel\n",
    "from huggingface_hub import notebook_login\n",
    "import textwrap\n",
    "import sys\n",
    "utils_path = os.path.abspath(\"../utils\")\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "import re\n",
    "from run_experiment import run_experiment\n",
    "from manage_folders import save_results_to_json, dataset_folders\n",
    "from retriever import DynamicRetriever\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b895a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Model$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbb733",
   "metadata": {},
   "source": [
    "##### $Model$ $Access$\n",
    "\n",
    "In order to access to the model we need to:\n",
    "1. Visit the webpage of the model: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n",
    "2. Log in to our HF account\n",
    "3. Read and Accept the Use Policies \n",
    "4. Write yout contact information (email and username)\n",
    "5. Make a request\n",
    "6. Run `from huggingface_hub import notebook_login` and `notebook_login()` \n",
    "7. Create a new token with `read` rights\n",
    "8. Copy the token and paste it in the notebook and press `Enter`\n",
    "\n",
    " *It might take some minutes*...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcadd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb939b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model name and configuration\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed722247",
   "metadata": {},
   "source": [
    "##### $Bnb$ $Configuration$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dfc9c9",
   "metadata": {},
   "source": [
    "\n",
    "The `bnb_config` creates a configuration to shrink the language model. \n",
    "\n",
    "* *Compresses the Model:* It tells the system to load the model in a \"compressed\" 4-bit format instead of its full 16-bit size.\n",
    "* *Saves Memory:* This makes the model about 4 times smaller, allowing it to run on computers with less memory (RAM and VRAM).\n",
    "* *Maintains Performance:* It uses clever tricks (like doing the actual math in 16-bit) to ensure the shrunken model is still fast and accurate.\n",
    "\n",
    "Essentially, it's a set of instructions to make a huge model fit on the computer with minimal loss in quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a960b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bee866",
   "metadata": {},
   "source": [
    "##### $Tokenizer$ $Set-up$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7c501",
   "metadata": {},
   "source": [
    "This code sets up a `tokenizer` for the language model.\n",
    "It loads a pre-trained tokenizer, makes sure there's a padding token (for making all inputs the same length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b9a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "# Set chat_template if missing\n",
    "if not hasattr(tokenizer, \"chat_template\") or tokenizer.chat_template is None:\n",
    "    print(\"Chat template not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daecbb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb5473",
   "metadata": {},
   "source": [
    "#### $System$ $Prompt$\n",
    "\n",
    "To ensure the model performs the intended taskâ€”such as question decompositionâ€”we provide clear and detailed instructions. This involves defining the modelâ€™s role, outlining the steps it should follow, and specifying the desired structure of the output. By being as explicit as possible, we guide the model toward producing consistent and accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7daef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an expert at breaking down complex questions into a numbered sequence of simple, factual sub-questions. \"\n",
    "    \"After listing all sub-questions DO NOT provide anything else.\\n\\n\"\n",
    "    \n",
    "    \"Follow this exact format:\\n\"\n",
    "    \"subq1: <sub-question>\\n\"\n",
    "    \"subq2: <sub-question>\\n\"\n",
    "    \"... continue numbering sequentially as needed ...\\n\"\n",
    "    \n",
    "    \"Do NOT repeat the original question. \"\n",
    "    \"Do NOT include commentary, explanations, assumptions, or any extra text. \"\n",
    "    \"Strictly adhere to the numbering and format above.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e86b625",
   "metadata": {},
   "source": [
    "#### $Parameters$ $for$ $the$ $Model$ $Configuration:$\n",
    "\n",
    "* **`max_new_tokens`**\n",
    "  Defines the maximum number of tokens the model can generate in a single response.\n",
    "\n",
    "  * Example: `max_new_tokens=512` means the model will not produce more than 512 tokens beyond the prompt, preventing it from generating endlessly.\n",
    "\n",
    "* **`do_sample`**\n",
    "  Controls whether the model generates text deterministically or stochastically.\n",
    "\n",
    "  * **`do_sample=False`** â†’ Deterministic: the model always picks the most likely next token, producing the same output for the same input. Useful for comparison and evaluation.\n",
    "  * **`do_sample=True`** â†’ Stochastic: the model samples from the probability distribution, allowing for more variety and creativity. Choices are influenced by parameters such as *temperature* and *top_p*.\n",
    "\n",
    "  ðŸ”¹ For **Mistral-7B-Instruct-v.03**, we use:\n",
    "\n",
    "  * `do_sample=True`\n",
    "  * `temperature=0.7`\n",
    "  * `top_p=0.9`\n",
    "\n",
    "<!-- * **`repetition_penalty`**\n",
    "  Adjusts the likelihood of repeating tokens to encourage more diverse output.\n",
    "\n",
    "  * Default: `1.0` (no penalty)\n",
    "  * Typical range: `1.1 â€“ 2.0`\n",
    "\n",
    "    * ~1.2 â†’ light penalty\n",
    "    * â‰¥1.5 â†’ strong penalty -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a78c2",
   "metadata": {},
   "source": [
    "#### $Attention$\n",
    "\n",
    "For **Mistral 7B**, there is **no official Hugging Face documentation** listing the recommended parameters. To access the official API and obtain recommended defaults, follow these steps:\n",
    "\n",
    "1. Sign in at [Mistral AI](https://mistral.ai) or directly at [https://console.mistral.ai/api-keys](https://console.mistral.ai/api-keys).\n",
    "2. Go to the **API Keys** section.\n",
    "3. Click on **Choose a plan** and subscribe to the **Free Plan**  \n",
    "   - Note: Phone number verification is required.\n",
    "4. Return to the **API Keys** section.\n",
    "5. Click on **Create New Key**.\n",
    "6. Copy the generated key and paste it in your code.\n",
    "\n",
    "\n",
    "#### $Additional$ $Hugging$ $Face$ $Reference$\n",
    "\n",
    "For the model [TheBloke/Mistral-7B-Instruct-v0.2-AWQ](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-AWQ), the recommended parameters are:\n",
    "\n",
    "- **temperature**: 0.7  \n",
    "- **top_p**: 0.9\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf7196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# API_KEY = \"Your_Mistral_API_Key_Here\"\n",
    "# BASE_URL = \"https://api.mistral.ai/v1\"\n",
    "\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {API_KEY}\"\n",
    "# }\n",
    "\n",
    "# response = requests.get(f\"{BASE_URL}/models\", headers=headers)\n",
    "\n",
    "# models = response.json()\n",
    "# models[\"data\"][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f96d7d",
   "metadata": {},
   "source": [
    "The list has the default temperature for the base version of `Mistral=7B`\n",
    "\n",
    "```java\n",
    "{'id': 'open-mistral-7b',\n",
    " 'object': 'model',\n",
    " 'created': 1756752489,\n",
    " 'owned_by': 'mistralai',\n",
    " 'capabilities': {'completion_chat': True,\n",
    "  'function_calling': True,\n",
    "  'completion_fim': False,\n",
    "  'fine_tuning': True,\n",
    "  'vision': False,\n",
    "  'ocr': False,\n",
    "  'classification': False,\n",
    "  'moderation': False,\n",
    "  'audio': False},\n",
    " 'name': 'open-mistral-7b',\n",
    " 'description': 'Our first dense model released September 2023.',\n",
    " 'max_context_length': 32768,\n",
    " 'aliases': ['mistral-tiny', 'mistral-tiny-2312'],\n",
    " 'deprecation': None,\n",
    " 'deprecation_replacement_model': None,\n",
    " 'default_model_temperature': 0.7,\n",
    " 'type': 'base'}\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model specific configuration\n",
    "model_config = {\n",
    "    \"model_id\": model_name,\n",
    "    \"uses_system_prompt\": True,  # Llama supports a system prompt\n",
    "    \"system_prompt\": system_prompt,\n",
    "    \"generation_params\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        # \"repetition_penalty\": 1.2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the model for our file names\n",
    "model_file_name = \"Mistral_7B_results.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03214c5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $QDMR$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7703de3c",
   "metadata": {},
   "source": [
    "We use a helper function `dataset_folders(base_results_folder, dataset_path, few_shot_examples_path)` that streamlines the setup process for running experiments on a new dataset. It handles three key tasks:\n",
    "\n",
    "1.  **Creates Output Directories:** It takes a base folder path and automatically creates the `zero_shot` and `few_shot` subdirectories where the model's predictions will be saved.\n",
    "2.  **Loads Evaluation Data:** It reads the main dataset file (e.g., `hotpot_dataset.json`) containing the questions to be evaluated.\n",
    "3.  **Loads Few-Shot Examples:** It reads the corresponding file containing the high-quality examples for few-shot prompting.\n",
    "\n",
    "The function returns a single, convenient dictionary containing all these assets (the loaded data and the output folder paths), which can then be easily used by the main experiment functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591aa23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdmr_base_folder = \"../QDMR/llm_predictions/static\"\n",
    "qdmr_dataset_file = \"../QDMR/QDMR_examples/qdmr_evaluation.json\" # Define the path to the QDMR dataset\n",
    "qdmr_fewshot_file = \"../QDMR/QDMR_examples/qdmr_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(qdmr_base_folder, qdmr_dataset_file, qdmr_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "qdmr_data_full = data_assets[\"data_full\"]\n",
    "qdmr_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0b73a",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1145ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "\n",
    "zero_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config,  \n",
    "    num_shots=0,\n",
    "    batch_size=5\n",
    ")\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe9704a",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Static$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"static\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa397b0",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Random$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b785a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdmr_base_folder = \"../QDMR/llm_predictions/random\"\n",
    "qdmr_dataset_file = \"../QDMR/QDMR_examples/qdmr_evaluation.json\" # Define the path to the QDMR dataset\n",
    "qdmr_fewshot_file = \"../QDMR/QDMR_examples/qdmr_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(qdmr_base_folder, qdmr_dataset_file, qdmr_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "qdmr_data_full = data_assets[\"data_full\"]\n",
    "qdmr_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Random-3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"random\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "\tbatch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f4c2a",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Dynamic$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdc8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdmr_base_folder = \"../QDMR/llm_predictions/dynamic\"\n",
    "qdmr_dataset_file = \"../QDMR/QDMR_examples/qdmr_evaluation.json\" # Define the path to the QDMR dataset\n",
    "qdmr_fewshot_file = \"../QDMR/QDMR_examples/qdmr_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(qdmr_base_folder, qdmr_dataset_file, qdmr_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "qdmr_data_full = data_assets[\"data_full\"]\n",
    "qdmr_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Dynamic-3-Shot Experiment\")\n",
    "\n",
    "retriever_instance = DynamicRetriever(shot_examples)\n",
    "\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=qdmr_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"dynamic\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=retriever_instance,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19673e21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $HotpotQA$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc11b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the dataset we want to use\n",
    "hotpot_base_folder = '../HotpotQA/llm_predictions/static/'\n",
    "hotpot_dataset_file = '../HotpotQA/HotpotQA_examples/hotpot_evaluation.json'\n",
    "hotpot_fewshot_file = '../HotpotQA/HotpotQA_examples/hotpot_few_shot.json'\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(hotpot_base_folder, hotpot_dataset_file, hotpot_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "hotpot_data_full = data_assets[\"data_full\"]\n",
    "hotpot_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20193968",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d8ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "\n",
    "zero_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config,  \n",
    "    num_shots=0,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0f56a",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Static$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"static\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4659bf",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Random$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the dataset we want to use\n",
    "hotpot_base_folder = '../HotpotQA/llm_predictions/random/'\n",
    "hotpot_dataset_file = '../HotpotQA/HotpotQA_examples/hotpot_evaluation.json'\n",
    "hotpot_fewshot_file = '../HotpotQA/HotpotQA_examples/hotpot_few_shot.json'\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(hotpot_base_folder, hotpot_dataset_file, hotpot_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "hotpot_data_full = data_assets[\"data_full\"]\n",
    "hotpot_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63bce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Random-3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"random\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664bd199",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Dynamic$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the dataset we want to use\n",
    "hotpot_base_folder = '../HotpotQA/llm_predictions/dynamic/'\n",
    "hotpot_dataset_file = '../HotpotQA/HotpotQA_examples/hotpot_evaluation.json'\n",
    "hotpot_fewshot_file = '../HotpotQA/HotpotQA_examples/hotpot_few_shot.json'\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(hotpot_base_folder, hotpot_dataset_file, hotpot_fewshot_file, few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "hotpot_data_full = data_assets[\"data_full\"]\n",
    "hotpot_data = data_assets[\"data_subset\"] # This is the subset of 5 examples for tuning\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8075f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Dynamic-3-Shot Experiment\")\n",
    "\n",
    "retriever_instance = DynamicRetriever(shot_examples)\n",
    "\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=hotpot_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"dynamic\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=retriever_instance,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "\tbatch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890902b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $StrategyQA$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec8fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "strategyqa_base_folder = '../StrategyQA/llm_predictions/static'\n",
    "strategyqa_dataset_file = \"../StrategyQA/StrategyQA_examples/strategyqa_evaluation.json\" # Define the path to the strategyqa dataset\n",
    "strategyqa_fewshot_file = \"../StrategyQA/StrategyQA_examples/strategyqa_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(strategyqa_base_folder, strategyqa_dataset_file, strategyqa_fewshot_file,  few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "strategyqa_data_full = data_assets[\"data_full\"]\n",
    "strategyqa_data = data_assets[\"data_subset\"] # This is the subset of 5 examples\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0397b",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shot$ $Experiment$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "\n",
    "zero_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config,  \n",
    "    num_shots=0,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88bf1cf",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Static$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"static\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf8f7f",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Random$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "strategyqa_base_folder = '../StrategyQA/llm_predictions/random'\n",
    "strategyqa_dataset_file = \"../StrategyQA/StrategyQA_examples/strategyqa_evaluation.json\" # Define the path to the strategyqa dataset\n",
    "strategyqa_fewshot_file = \"../StrategyQA/StrategyQA_examples/strategyqa_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(strategyqa_base_folder, strategyqa_dataset_file, strategyqa_fewshot_file,  few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "strategyqa_data_full = data_assets[\"data_full\"]\n",
    "strategyqa_data = data_assets[\"data_subset\"] # This is the subset of 5 examples\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e734a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Random-3-Shot Experiment\")\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"random\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=None,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a792515",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Experiment$ $-$ $Dynamic$ $Shots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64104f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "strategyqa_base_folder = '../StrategyQA/llm_predictions/dynamic'\n",
    "strategyqa_dataset_file = \"../StrategyQA/StrategyQA_examples/strategyqa_evaluation.json\" # Define the path to the strategyqa dataset\n",
    "strategyqa_fewshot_file = \"../StrategyQA/StrategyQA_examples/strategyqa_few_shot.json\"\n",
    "\n",
    "# Call the function to get everything we need in one go\n",
    "data_assets = dataset_folders(strategyqa_base_folder, strategyqa_dataset_file, strategyqa_fewshot_file,  few_shot_type=\"static\", tuning_subset_size=5)\n",
    "\n",
    "# Unpack the dictionary into these variables for easy access\n",
    "strategyqa_data_full = data_assets[\"data_full\"]\n",
    "strategyqa_data = data_assets[\"data_subset\"] # This is the subset of 5 examples\n",
    "shot_examples = data_assets[\"shot_examples\"]\n",
    "zero_shot_folder = data_assets[\"zero_shot_folder\"]\n",
    "few_shot_folder = data_assets[\"few_shot_folder\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting Dynamic-3-Shot Experiment\")\n",
    "\n",
    "retriever_instance = DynamicRetriever(shot_examples)\n",
    "\n",
    "# Run a 3-shot experiment\n",
    "three_shot_results = run_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=strategyqa_data,\n",
    "    shot_examples=shot_examples,\n",
    "    model_config=model_config, # Use the same config\n",
    "    num_shots=3,\n",
    "    few_shot_type=\"dynamic\",  # \"static\" | \"random\" | \"dynamic\"\n",
    "    retriever=retriever_instance,          # Required if few_shot_type=\"dynamic\"\n",
    "    seed=42,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Save the results with a different filename\n",
    "save_results_to_json(three_shot_results, few_shot_folder, f\"3shot_{model_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
