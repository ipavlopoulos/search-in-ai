{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4124e23a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Libraries$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00886bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from huggingface_hub import notebook_login\n",
    "import textwrap\n",
    "import re\n",
    "# !pip install -U transformers accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feac4b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Model$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c0a84",
   "metadata": {},
   "source": [
    "##### $Model$ $Access$\n",
    "\n",
    "In order to access to the model we need to:\n",
    "1. Visit the webpage of the model: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "2. Log in to our HF account\n",
    "3. Read and Accept the Use Policies of Meta \n",
    "4. Write yout contact information (email and username)\n",
    "5. Make a request\n",
    "6. An email for validation will be sent\n",
    "7. Run `from huggingface_hub import notebook_login` and `notebook_login()` \n",
    "8. Press the link shown. It will take you to HF and create a new token with `read` rights\n",
    "9. Copy the token and paste it in the notebook and press `Enter`\n",
    "\n",
    " *It might take some minutes*...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94634675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c8510e7be440de8a6abb595ef0d266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e679c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model name and configuration\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ecfa2",
   "metadata": {},
   "source": [
    "##### $Bnb$ $Configuration$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c81f12",
   "metadata": {},
   "source": [
    "\n",
    "The `bnb_config` creates a configuration to shrink the language model. \n",
    "\n",
    "* *Compresses the Model:* It tells the system to load the model in a \"compressed\" 4-bit format instead of its full 16-bit size.\n",
    "* *Saves Memory:* This makes the model about 4 times smaller, allowing it to run on computers with less memory (RAM and VRAM).\n",
    "* *Maintains Performance:* It uses clever tricks (like doing the actual math in 16-bit) to ensure the shrunken model is still fast and accurate.\n",
    "\n",
    "Essentially, it's a set of instructions to make a huge model fit on the computer with minimal loss in quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e18ee5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4d454",
   "metadata": {},
   "source": [
    "##### $Tokenizer$ $Set-up$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d71c3",
   "metadata": {},
   "source": [
    "This code sets up a `tokenizer` for the language model.\n",
    "It loads a pre-trained tokenizer, makes sure there's a padding token (for making all inputs the same length), and if no chat template is found, it manually adds one specifically for Llama 3.x models to format conversations correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84612219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2db3413a104aaf89fb10bab06beb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0320ac37e0c43d19568ecb2ab723e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f8c65507e449d381f089fa6c674d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set chat_template if missing\n",
    "if not hasattr(tokenizer, \"chat_template\") or tokenizer.chat_template is None:\n",
    "    print(\"Chat template not found. Setting LLaMA 3.1 chat template manually.\")\n",
    "    tokenizer.chat_template = (\n",
    "        \"<|begin_of_text|>{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'user' %}\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
    "        \"{% endif %}\"\n",
    "        \"{% endfor %}\"\n",
    "        \"{% if add_generation_prompt %}\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        \"{% endif %}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e02ae1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "353ae3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dd3ff9194248a4b1e8d708c1a21cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788b4b3ee0364641bc0403809a9a166e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87421fe6ea9e4b5ab57417afa3f0e70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b462d9b17341879460412beb077a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb4647d7877487e87c649a5cf96b7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3e5e67b1aa426d9cee7349b19537c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de21d086b7447bd85ffb98bdd3e2600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb97e9f91d4b4ec785af64a9b2bcbd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca574cdd8fba44fba3fbae6385d5b90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # or torch.bfloat16 if supported\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbc2f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "####  $Zero$ $Shots$ $vs.$ $Few$ $Shots$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cabeb3",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shots$ $Function$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5b788",
   "metadata": {},
   "source": [
    "For each question, it constructs a prompt instructing the model to break down the complex question into smaller, step-by-step sub-questions. It then uses the tokenizer to convert these messages into input IDs, adds an attention mask, and generates a response from the model. Finally, it decodes the model's output and stores the original question and its zero-shot decomposition. The function returns a list of dictionaries containing these results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d69a3",
   "metadata": {},
   "source": [
    "* **`input_ids`**: This is the prompt, but translated into a numerical format (a list of numbers) that the model can understand. Each number corresponds to a word or part of a word.\n",
    "\n",
    "* **`attention_mask`**: This is a list of 1s and 0s that has the same length as the input_ids. It tells the model which tokens are real words to pay attention to (a 1) and which ones are just padding that should be ignored (a 0). \n",
    "\n",
    "* **`pad_token_id`**: This specifies the ID of the special token that is used to \"fill up\" shorter sentences when we process multiple sentences at once (batching). It's directly related to the attention_mask.\n",
    "\n",
    "* **`max_new_tokens`**: This sets the maximum length of the generated response. max_new_tokens=512 tells the model, \"Do not write more than 512 new tokens after the prompt.\" This prevents it from writing forever.\n",
    "\n",
    "* **`early_stopping`**: If set to False, the model might finish its thought in 80 words but feel compelled to keep adding filler words to get closer to the 300-word limit. With early stopping once it writes its thought is complete, it just stops.\n",
    "\n",
    "\n",
    "* **`do_sample`**:\n",
    "\t* **do_sample=False**: This forces the model to be deterministic. Every time it generates a new word, it chooses the single word that it calculates as being the most statistically likely to come next. When we compare the models, we want to compare their \"best, most probable\" attempt at the problem.\n",
    "\t* **do_sample=True**: This tells the model to be creative and less predictable. Instead of always picking the #1 most likely word, it might pick the #2 or #3 most likely word, based on a random sample (controlled by parameters like temperature).\n",
    "\n",
    "Since the task is analytical (decomposing a problem) and not creative, the deterministic approach is better. We choose do_sample=False for all the baseline experiments to ensure your results are stable and repeatable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b190106",
   "metadata": {},
   "source": [
    "*For the prompt, we use the prompt structure the model has been trained to.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a10967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_zero_shot_experiment(model, tokenizer, questions):\n",
    "    results = []\n",
    "    for item in questions:\n",
    "        print(f\"Processing (Zero-Shot) ID: {item['id']}\")\n",
    "\n",
    "        system_instructions = (\n",
    "            \"You are an expert system that decomposes complex user questions into a numbered list of simple, sequential sub-questions.\"\n",
    "        )\n",
    "        user_question = item['question']\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_instructions},\n",
    "            {\"role\": \"user\", \"content\": user_question}\n",
    "        ]\n",
    "\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Add attention mask\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        # Generate the response\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.2,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        decomposition_result = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "        # Store the result\n",
    "        results.append({\n",
    "            \"id\": item['id'],\n",
    "            \"question\": item['question'],\n",
    "            \"decomposition\": item['decomposition'],\n",
    "            \"zero_shot_decomposition\": decomposition_result\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f9026",
   "metadata": {},
   "source": [
    "##### $Few$ $Shots$ $Function$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a2916d",
   "metadata": {},
   "source": [
    "This function, performs few-shots learning.\n",
    "\n",
    "It takes a list of questions, a list of high-quality example question/decomposition pairs (`shot_examples`), and the number of examples (`num_shots`) to use.\n",
    "\n",
    "The function first uses a prompt that includes the specified number of examples, showing the model how to decompose complex questions into simpler sub-questions. Then, for each question, it sends the prompt to the language model, generates a decomposition, and stores the results. This approach helps the model understand the desired output format and style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeaca19",
   "metadata": {},
   "source": [
    "*For the prompt, we use the prompt structure the model has been trained to.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97c5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_few_shot_experiment(model, tokenizer, data, shot_examples, num_shots=3):\n",
    "    \n",
    "    if num_shots > len(shot_examples):\n",
    "        raise ValueError(f\"You asked for {num_shots} shots, but only {len(shot_examples)} are available.\")\n",
    "\n",
    "    results = []\n",
    "    for item in data:\n",
    "        print(f\"Processing ({num_shots}-Shot) ID: {item['id']}\")\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert assistant. Your task is to break down the following question into a few smaller questions that contribute to solving the overall problem. Give only the decomposition steps, not the final answer.\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Add the few-shot examples as a series of user/assistant turns\n",
    "        for example in shot_examples[:num_shots]:\n",
    "            messages.append({\"role\": \"user\", \"content\": example['question']})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": example['decomposition']})\n",
    "\n",
    "        # Finally, add the actual user question we want the model to answer now\n",
    "        messages.append({\"role\": \"user\", \"content\": item['question']})\n",
    "\n",
    "        # Generate the Response\n",
    "        \n",
    "        # The input to apply_chat_template is now the structured 'messages' list\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Add attention mask\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        # Explicitly set pad_token_id\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.2,  \n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        generated_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "        decomposition_result = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        # Store the result \n",
    "        results.append({\n",
    "            \"id\": item['id'],\n",
    "            \"question\": item['question'],\n",
    "            \"decomposition\": item['decomposition'],\n",
    "            f\"{num_shots}_shot_decomposition\": decomposition_result\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630c6ed",
   "metadata": {},
   "source": [
    "##### $Save$ $the$ $results$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d566460",
   "metadata": {},
   "source": [
    "*Once the model processes the questions, its predictions for the zero-shot and few-shot experiments are saved in the corresponding folders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9d84db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_json(results, folder, filename):\n",
    "\n",
    "    # Make sure the folder exists\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Construct the full path for the file\n",
    "    full_path = os.path.join(folder, filename)\n",
    "\n",
    "    # Save the file\n",
    "    with open(full_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Results have been saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73fba45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the model for our file names\n",
    "model_file_name = \"Llama-3.1-8B_Hotpot_results.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a7197",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $HotpotQA$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ac007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "base_results_folder = '../HotpotQA/llm_predictions/'\n",
    "zero_shot_folder = os.path.join(base_results_folder, 'zero_shot') # Zero-shot predictions\n",
    "few_shots_folder = os.path.join(base_results_folder, 'few_shot')  # Few-shot predictions\n",
    "os.makedirs(zero_shot_folder, exist_ok=True)\n",
    "os.makedirs(few_shots_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64b1ce8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "hotpot_dataset_path = \"../HotpotQA/HotpotQA_dataset/hotpot_dataset.json\" # Define the path to the hotpot dataset\n",
    "\n",
    "# Load the dataset\n",
    "with open(hotpot_dataset_path, \"r\") as file:\n",
    "    hotpot_data = json.load(file)\n",
    "    print(f\"Loaded {len(hotpot_data)} questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d6b26fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First question details:\n",
      "ID: 5a8b57f25542995d1e6f1371\n",
      "Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "Answer: yes\n",
      "Supporting sentences: ['Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.', 'Edward Davis Wood Jr. (October 10, 1924 – December 10, 1978) was an American filmmaker, actor, writer, producer, and director.', 'Aggregating the above we conclude that the answer is: yes']\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Decomposition: ['What nationality Scott Derrickson had?', 'What nationality Ed Wood had?', 'Was the nationality the same?']\n"
     ]
    }
   ],
   "source": [
    "# Display the first question\n",
    "print(\"First question details:\")\n",
    "print(\"ID:\", hotpot_data[0][\"id\"])\n",
    "print(\"Question:\", hotpot_data[0][\"question\"])\n",
    "print(\"Answer:\", hotpot_data[0][\"answer\"])\n",
    "print(\"Supporting sentences:\", hotpot_data[0][\"supporting_sentences\"])\n",
    "print(\"-\"*150)\n",
    "print(\"Decomposition:\", hotpot_data[0][\"decomposition\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f53a2f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "few_shot_hotpot_examples_path = \"../HotpotQA/HotpotQA_dataset/hotpot_few_shot.json\"\n",
    "with open(few_shot_hotpot_examples_path, \"r\") as file:\n",
    "    shot_examples = json.load(file)\n",
    "    print(f\"Loaded {len(hotpot_data)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d8bf85",
   "metadata": {},
   "source": [
    "##### $Experiments$ $Run$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afaad120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Zero-Shot Experiment\n",
      "Processing (Zero-Shot) ID: 5a8b57f25542995d1e6f1371\n",
      "Processing (Zero-Shot) ID: 5a8c7595554299585d9e36b6\n",
      "Processing (Zero-Shot) ID: 5a85ea095542994775f606a8\n",
      "Processing (Zero-Shot) ID: 5adbf0a255429947ff17385a\n",
      "Processing (Zero-Shot) ID: 5a8e3ea95542995a26add48d\n",
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "zero_shot_results = run_zero_shot_experiment(model, tokenizer, hotpot_data)\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b35eb253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 3-Shot Experiment\n",
      "Processing (3-Shot) ID: 5a8b57f25542995d1e6f1371\n",
      "Processing (3-Shot) ID: 5a8c7595554299585d9e36b6\n",
      "Processing (3-Shot) ID: 5a85ea095542994775f606a8\n",
      "Processing (3-Shot) ID: 5adbf0a255429947ff17385a\n",
      "Processing (3-Shot) ID: 5a8e3ea95542995a26add48d\n",
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "three_shot_results = run_few_shot_experiment(model, tokenizer, hotpot_data, shot_examples, num_shots=3)\n",
    "save_results_to_json(three_shot_results, few_shots_folder, f\"3shot_{model_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b863ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $QDMR$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b1c4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "base_results_folder = '../QDMR/llm_predictions/'\n",
    "zero_shot_folder = os.path.join(base_results_folder, 'zero_shot') # Zero-shot predictions\n",
    "few_shots_folder = os.path.join(base_results_folder, 'few_shot')  # Few-shot predictions\n",
    "os.makedirs(zero_shot_folder, exist_ok=True)\n",
    "os.makedirs(few_shots_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9234497f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "qdmr_dataset_path = \"../QDMR/QDMR_dataset/qdmr_dataset.json\" # Define the path to the qdmr dataset\n",
    "\n",
    "# Load the dataset\n",
    "with open(qdmr_dataset_path, \"r\") as file:\n",
    "    qdmr_data = json.load(file)\n",
    "    print(f\"Loaded {len(qdmr_data)} questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7b78c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First question details:\n",
      "ID: CWQ_dev_WebQTest-1011_c0be4f76a5397ba6d0d06f53905e504b\n",
      "Question: What Tibetan speaking countries have a population of less than 993885000?\n",
      "Lexicon tokens: ['higher than', 'same as', 'what ', 'and ', 'than ', 'at most', 'distinct', 'two', 'at least', 'or ', 'date', 'on ', '@@14@@', 'countries', 'equal', 'hundred', 'those', 'sorted by', 'elevation', 'which ', '@@6@@', '993885000', 'was ', 'did ', 'population', 'height', 'one', 'that ', 'on', 'did', 'who', 'true', '@@2@@', '100', 'false', 'and', 'was', 'speaking', 'populations', 'who ', 'a ', 'the', 'number of ', '@@16@@', 'if ', 'where', '@@18@@', 'how', 'larger than', 'is ', 'from ', 'a', 'less', 'for each', 'are ', '@@19@@', '@@4@@', '@@11@@', 'distinct ', 'to', 'not ', 'objects', 'with ', ', ', 'lowest', 'in', 'has ', 'zero', 'in ', 'there ', 'lower than', 'highest', '@@9@@', 'than', 'size', 'multiplication', 'with', 'besides ', ',', '@@1@@', 'what', 'have', 'those ', 'of', '@@3@@', 'that', 'there', '@@10@@', '@@5@@', 'both ', '@@15@@', 'number of', 'price', 'any', 'which', 'Tibetan', 'to ', 'how ', 'when ', 'of ', 'division', 'country', 'is', 'sum', 'or', 'if', 'more', '@@12@@', 'smaller than', 'flights', '@@7@@', '@@17@@', 'for each ', 'What', 'speak', 'from', '@@13@@', 'has', 'difference', 'when', 'are', 'any ', '@@8@@', 'both', 'the ', ',  ', 'besides', 'have ', 'where ', 'not']\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Decomposition: return Tibetan speaking countries\n",
      "return #1 that have a population of less than 993885000\n"
     ]
    }
   ],
   "source": [
    "# Display the first question\n",
    "print(\"First question details:\")\n",
    "print(\"ID:\", qdmr_data[0][\"id\"])\n",
    "print(\"Question:\", qdmr_data[0][\"question\"])\n",
    "print(\"Lexicon tokens:\", qdmr_data[0][\"lexicon tokens\"])\n",
    "print(\"-\"*150)\n",
    "print(\"Decomposition:\", qdmr_data[0][\"decomposition\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a8b50ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "few_shot_qdmr_examples_path = \"../QDMR/QDMR_dataset/qdmr_few_shot.json\"\n",
    "with open(few_shot_qdmr_examples_path, \"r\") as file:\n",
    "    shot_examples = json.load(file)\n",
    "    print(f\"Loaded {len(qdmr_data)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b631538",
   "metadata": {},
   "source": [
    "##### $Experiments$ $Run$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2491280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Zero-Shot Experiment\n",
      "Processing (Zero-Shot) ID: CWQ_dev_WebQTest-1011_c0be4f76a5397ba6d0d06f53905e504b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lathanasopoulou/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/lathanasopoulou/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:641: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (Zero-Shot) ID: CWQ_dev_WebQTest-1011_edc922a0faa1e47614eb7e6effe2d1a1\n",
      "Processing (Zero-Shot) ID: CWQ_dev_WebQTest-1036_0b5333d98ef87008aa02d1fbc1554b05\n",
      "Processing (Zero-Shot) ID: CWQ_dev_WebQTest-1036_4e73509d14bda62590480b655eee8751\n",
      "Processing (Zero-Shot) ID: CWQ_dev_WebQTest-1081_1ecabf57357cb4abd089a4af52154854\n",
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "zero_shot_results = run_zero_shot_experiment(model, tokenizer, qdmr_data)\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aacf530a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 3-Shot Experiment\n",
      "Processing (3-Shot) ID: CWQ_dev_WebQTest-1011_c0be4f76a5397ba6d0d06f53905e504b\n",
      "Processing (3-Shot) ID: CWQ_dev_WebQTest-1011_edc922a0faa1e47614eb7e6effe2d1a1\n",
      "Processing (3-Shot) ID: CWQ_dev_WebQTest-1036_0b5333d98ef87008aa02d1fbc1554b05\n",
      "Processing (3-Shot) ID: CWQ_dev_WebQTest-1036_4e73509d14bda62590480b655eee8751\n",
      "Processing (3-Shot) ID: CWQ_dev_WebQTest-1081_1ecabf57357cb4abd089a4af52154854\n",
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "three_shot_results = run_few_shot_experiment(model, tokenizer, qdmr_data, shot_examples, num_shots=3)\n",
    "save_results_to_json(three_shot_results, few_shots_folder, f\"3shot_{model_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
