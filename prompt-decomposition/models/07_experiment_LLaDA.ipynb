{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0df56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Libraries$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cddfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModel\n",
    "from huggingface_hub import notebook_login\n",
    "import textwrap\n",
    "import re\n",
    "from generate import generate\n",
    "# !pip install -U transformers accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d58f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Model$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eb67dc",
   "metadata": {},
   "source": [
    "##### $Model$ $Access$\n",
    "\n",
    "In order to access to the model we need to:\n",
    "1. Visit the github of the model: https://github.com/ML-GSAI/LLaDA?tab=readme-ov-file\n",
    "2. Clone it\n",
    "3. Create inside the folder the notebook and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b1364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model name \n",
    "model_name = 'GSAI-ML/LLaDA-8B-Base'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a2dc4",
   "metadata": {},
   "source": [
    "##### $Tokenizer$ $Set-up$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd935e",
   "metadata": {},
   "source": [
    "This code sets up a `tokenizer` for the language model.\n",
    "It loads a pre-trained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00b592a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5982b732574e13829ea474dd9764f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2ec7ae7efc47d2a5a43eba563e3a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3a9d01e9f9407cb111efc24370c526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/766 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for GSAI-ML/LLaDA-8B-Base loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer. The library will handle the chat template automatically.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "print(f\"Tokenizer for {model_name} loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd895d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce2e1a4f5a4422e96c2324f77ae8fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e5c956a83244f5b7ddcb098d18495c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_llada.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Base:\n",
      "- configuration_llada.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29bbf20c54e42ac9af3a24e6f793bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_llada.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Base:\n",
      "- modeling_llada.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dee619399e041229e8c9e1270bb3431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3498e5ab56524d31b521cde4e760354e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2c608fd7474b3d8876068c1bebc535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c937edb2f20b460bafaea6dc3a9ba287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/2.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ed848b472f4725994b099484cff51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/2.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1884ea07537c4dfa91fa737620448fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cebc39ee871413e8274f3efabc53ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cad43210aa4bfb91ea4422e0af811b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90955ce47c22434eb6359b99ebd09def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730b1489a98b481594a23a02daf2ffcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # or torch.bfloat16 if supported\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae10d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n"
     ]
    }
   ],
   "source": [
    "print(chat_template := tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d865d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "####  $Zero$ $Shots$ $vs.$ $Few$ $Shots$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder paths for results and the folders if they do not exist\n",
    "base_results_folder = '/home/lathanasopoulou/capstone/search-in-ai/prompt-decomposition/HotpotQA/llm_predictions/'\n",
    "zero_shot_folder = os.path.join(base_results_folder, 'zero_shot') # Zero-shot predictions\n",
    "few_shots_folder = os.path.join(base_results_folder, 'few_shot')  # Few-shot predictions\n",
    "os.makedirs(zero_shot_folder, exist_ok=True)\n",
    "os.makedirs(few_shots_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29185c5",
   "metadata": {},
   "source": [
    "##### $Zero$ $Shots$ $Function$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e119e80",
   "metadata": {},
   "source": [
    "For each question, it constructs a prompt instructing the model to break down the complex question into smaller, step-by-step sub-questions. It then uses the tokenizer to convert these messages into input IDs, adds an attention mask, and generates a response from the model. Finally, it decodes the model's output and stores the original question and its zero-shot decomposition. The function returns a list of dictionaries containing these results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8d769",
   "metadata": {},
   "source": [
    "* **`input_ids`**: This is the prompt, but translated into a numerical format (a list of numbers) that the model can understand. Each number corresponds to a word or part of a word.\n",
    "\n",
    "* **`attention_mask`**: This is a list of 1s and 0s that has the same length as the input_ids. It tells the model which tokens are real words to pay attention to (a 1) and which ones are just padding that should be ignored (a 0). \n",
    "\n",
    "* **`pad_token_id`**: This specifies the ID of the special token that is used to \"fill up\" shorter sentences when we process multiple sentences at once (batching). It's directly related to the attention_mask.\n",
    "\n",
    "* **`max_new_tokens`**: This sets the maximum length of the generated response. max_new_tokens=512 tells the model, \"Do not write more than 512 new tokens after the prompt.\" This prevents it from writing forever.\n",
    "\n",
    "* **`early_stopping`**: If set to False, the model might finish its thought in 80 words but feel compelled to keep adding filler words to get closer to the 300-word limit. With early stopping once it writes its thought is complete, it just stops.\n",
    "\n",
    "\n",
    "* **`do_sample`**:\n",
    "\t* **do_sample=False**: This forces the model to be deterministic. Every time it generates a new word, it chooses the single word that it calculates as being the most statistically likely to come next. When we compare the models, we want to compare their \"best, most probable\" attempt at the problem.\n",
    "\t* **do_sample=True**: This tells the model to be creative and less predictable. Instead of always picking the #1 most likely word, it might pick the #2 or #3 most likely word, based on a random sample (controlled by parameters like temperature).\n",
    "\n",
    "Since the task is analytical (decomposing a problem) and not creative, the deterministic approach is better. We choose do_sample=False for all the baseline experiments to ensure your results are stable and repeatable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb7a6a",
   "metadata": {},
   "source": [
    "*For the prompt, we use the prompt structure the model has been trained to.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78461188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_zero_shot_experiment(model, tokenizer, questions):\n",
    "    results = []\n",
    "\n",
    "    for item in questions:\n",
    "        print(f\"Processing (Zero-Shot) ID: {item['id']}\")\n",
    "\n",
    "        # Manually construct the prompt using your template format\n",
    "        system_prompt = (\n",
    "            \"You are a helpful assistant. Your task is to break down the following question \"\n",
    "            \"into a few smaller questions that contribute to solving the overall problem.\\n\\n\"\n",
    "            f\"Complex Question: {item['question']}\\n\\n\"\n",
    "            \"Step-by-Step plan:\"\n",
    "        )\n",
    "\n",
    "        # Manually format chat messages using the template provided\n",
    "        bos_token = tokenizer.bos_token or \"<|begin_of_text|>\"\n",
    "        eot_token = \"<|eot_id|>\"\n",
    "        start_header = \"<|start_header_id|>\"\n",
    "        end_header = \"<|end_header_id|>\"\n",
    "\n",
    "        prompt_text = (\n",
    "            f\"{bos_token}\"\n",
    "            f\"{start_header}user{end_header}\\n\\n{system_prompt.strip()}{eot_token}\"\n",
    "            f\"{start_header}assistant{end_header}\\n\\n\"\n",
    "        )\n",
    "\n",
    "        # Tokenize and convert to tensor\n",
    "        input_ids = tokenizer(prompt_text, return_tensors=\"pt\")['input_ids'].to(model.device)\n",
    "\n",
    "        # Run generation (assuming a LLaDA-specific generate function)\n",
    "        output_ids = generate(\n",
    "            model=model,\n",
    "            prompt=input_ids,\n",
    "            steps=128,\n",
    "            gen_length=128,\n",
    "            block_length=32,\n",
    "            temperature=0.0,\n",
    "            cfg_scale=0.0,\n",
    "            remasking='low_confidence'\n",
    "        )\n",
    "\n",
    "        # Decode only the newly generated tokens\n",
    "        generated_text = tokenizer.batch_decode(\n",
    "            output_ids[:, input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "\n",
    "        # Store the result\n",
    "        results.append({\n",
    "            \"id\": item['id'],\n",
    "            \"question\": item['question'],\n",
    "            \"decomposition\": item.get('decomposition', ''),\n",
    "            \"zero_shot_decomposition\": generated_text\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14baba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_zero_shot_experiment(model, tokenizer, questions):\n",
    "#     results = []\n",
    "\n",
    "#     for item in questions:\n",
    "#         print(f\"Processing (Zero-Shot) ID: {item['id']}\")\n",
    "\n",
    "#         # Construct the raw string prompt\n",
    "#         prompt_text = (\n",
    "#             \"You are a helpful assistant. Your task is to break down the following question \"\n",
    "#             \"into a few smaller questions that contribute to solving the overall problem.\\n\\n\"\n",
    "#             f\"Complex Question: {item['question']}\\n\\n\"\n",
    "#             \"Step-by-Step plan:\"\n",
    "#         )\n",
    "\n",
    "#         # Apply chat template if needed (LLaDA-Instruct models require it)\n",
    "#         messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "#         formatted_prompt = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             add_generation_prompt=True,\n",
    "#             tokenize=False\n",
    "#         )\n",
    "\n",
    "#         # Tokenize into input IDs\n",
    "#         input_ids = tokenizer(formatted_prompt)['input_ids']\n",
    "#         input_ids = torch.tensor(input_ids).to(model.device).unsqueeze(0)  # Shape: (1, L)\n",
    "\n",
    "#         # Run LLaDA's generation function\n",
    "#         output_ids = generate(\n",
    "#             model=model,\n",
    "#             prompt=input_ids,\n",
    "#             steps=128,\n",
    "#             gen_length=128,\n",
    "#             block_length=32,\n",
    "#             temperature=0.0,\n",
    "#             cfg_scale=0.0,\n",
    "#             remasking='low_confidence'\n",
    "#         )\n",
    "\n",
    "#         # Decode the generated output after the prompt\n",
    "#         generated_text = tokenizer.batch_decode(output_ids[:, input_ids.shape[1]:], skip_special_tokens=True)[0].strip()\n",
    "\n",
    "#         # Store the result\n",
    "#         results.append({\n",
    "#             \"id\": item['id'],\n",
    "#             \"question\": item['question'],\n",
    "#             \"decomposition\": item.get('decomposition', ''),  # fallback in case not provided\n",
    "#             \"zero_shot_decomposition\": generated_text\n",
    "#         })\n",
    "\n",
    "#     return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87bbc71",
   "metadata": {},
   "source": [
    "##### $Few$ $Shots$ $Function$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e87b9",
   "metadata": {},
   "source": [
    "This function, performs few-shots learning.\n",
    "\n",
    "It takes a list of questions, a list of high-quality example question/decomposition pairs (`shot_examples`), and the number of examples (`num_shots`) to use.\n",
    "\n",
    "The function first uses a prompt that includes the specified number of examples, showing the model how to decompose complex questions into simpler sub-questions. Then, for each question, it sends the prompt to the language model, generates a decomposition, and stores the results. This approach helps the model understand the desired output format and style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from generate import generate  # LLaDA's custom generate function\n",
    "\n",
    "# def run_few_shot_experiment(model, tokenizer, data, shot_examples, num_shots=3):\n",
    "#     if num_shots > len(shot_examples):\n",
    "#         raise ValueError(f\"You asked for {num_shots} shots, but only {len(shot_examples)} are available.\")\n",
    "\n",
    "#     results = []\n",
    "\n",
    "#     for item in data:\n",
    "#         print(f\"Processing ({num_shots}-Shot) ID: {item['id']}\")\n",
    "\n",
    "#         # Build the chat-style prompt using few-shot examples\n",
    "#         messages = [\n",
    "#             {\n",
    "#                 \"role\": \"system\",\n",
    "#                 \"content\": \"You are an expert assistant. Your task is to break down the following question into a few smaller questions that contribute to solving the overall problem. Give only the decomposition steps, not the final answer.\"\n",
    "#             }\n",
    "#         ]\n",
    "\n",
    "#         # Add few-shot examples\n",
    "#         for example in shot_examples[:num_shots]:\n",
    "#             decomposition_text = example['decomposition']\n",
    "#             if isinstance(decomposition_text, list):\n",
    "#                 decomposition_text = \"\\n\".join(decomposition_text)\n",
    "#             messages.append({\"role\": \"user\", \"content\": example['question']})\n",
    "#             messages.append({\"role\": \"assistant\", \"content\": decomposition_text})\n",
    "\n",
    "#         # Add the actual question to be decomposed\n",
    "#         messages.append({\"role\": \"user\", \"content\": item['question']})\n",
    "\n",
    "#         # Convert chat messages into a prompt string (not tokenized yet)\n",
    "#         prompt_text = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             add_generation_prompt=True,\n",
    "#             tokenize=False\n",
    "#         )\n",
    "\n",
    "#         # Tokenize to tensor (batch of size 1)\n",
    "#         input_ids = tokenizer(prompt_text)['input_ids']\n",
    "#         input_ids = torch.tensor(input_ids).to(model.device).unsqueeze(0)\n",
    "\n",
    "#         # Generate output using LLaDA\n",
    "#         output_ids = generate(\n",
    "#             model=model,\n",
    "#             prompt=input_ids,\n",
    "#             steps=128,\n",
    "#             gen_length=128,\n",
    "#             block_length=32,\n",
    "#             temperature=0.0,\n",
    "#             cfg_scale=0.0,\n",
    "#             remasking='low_confidence'\n",
    "#         )\n",
    "\n",
    "#         # Decode only the generated portion\n",
    "#         generated_text = tokenizer.batch_decode(\n",
    "#             output_ids[:, input_ids.shape[1]:],\n",
    "#             skip_special_tokens=True\n",
    "#         )[0].strip()\n",
    "\n",
    "#         results.append({\n",
    "#             \"id\": item['id'],\n",
    "#             \"question\": item['question'],\n",
    "#             \"decomposition\": item.get('decomposition', ''),\n",
    "#             f\"{num_shots}_shot_decomposition\": generated_text\n",
    "#         })\n",
    "\n",
    "#     return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_few_shot_experiment(model, tokenizer, data, shot_examples, num_shots=3):\n",
    "    if num_shots > len(shot_examples):\n",
    "        raise ValueError(f\"You asked for {num_shots} shots, but only {len(shot_examples)} are available.\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Special tokens\n",
    "    bos_token = tokenizer.bos_token or \"<|begin_of_text|>\"\n",
    "    eot_token = \"<|eot_id|>\"\n",
    "    start_header = \"<|start_header_id|>\"\n",
    "    end_header = \"<|end_header_id|>\"\n",
    "\n",
    "    for item in data:\n",
    "        print(f\"Processing ({num_shots}-Shot) ID: {item['id']}\")\n",
    "\n",
    "        # Begin with system message\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert assistant. Your task is to break down the following question into a few smaller questions that contribute to solving the overall problem. Give only the decomposition steps, not the final answer.\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Add few-shot examples\n",
    "        for example in shot_examples[:num_shots]:\n",
    "            decomposition_text = example['decomposition']\n",
    "            if isinstance(decomposition_text, list):\n",
    "                decomposition_text = \"\\n\".join(decomposition_text)\n",
    "\n",
    "            messages.append({\"role\": \"user\", \"content\": example['question']})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": decomposition_text})\n",
    "\n",
    "        # Add the new user question to decompose\n",
    "        messages.append({\"role\": \"user\", \"content\": item['question']})\n",
    "\n",
    "        # Manually build the prompt\n",
    "        prompt_parts = []\n",
    "        for idx, message in enumerate(messages):\n",
    "            role = message[\"role\"]\n",
    "            content = message[\"content\"].strip()\n",
    "            segment = f\"{start_header}{role}{end_header}\\n\\n{content}{eot_token}\"\n",
    "            if idx == 0:\n",
    "                segment = bos_token + segment  # Only add BOS once\n",
    "            prompt_parts.append(segment)\n",
    "\n",
    "        # Add assistant header for generation\n",
    "        prompt_parts.append(f\"{start_header}assistant{end_header}\\n\\n\")\n",
    "        prompt_text = \"\".join(prompt_parts)\n",
    "\n",
    "        # Tokenize\n",
    "        input_ids = tokenizer(prompt_text, return_tensors=\"pt\")['input_ids'].to(model.device)\n",
    "\n",
    "        # Generate output using LLaDA\n",
    "        output_ids = generate(\n",
    "            model=model,\n",
    "            prompt=input_ids,\n",
    "            steps=128,\n",
    "            gen_length=128,\n",
    "            block_length=32,\n",
    "            temperature=0.0,\n",
    "            cfg_scale=0.0,\n",
    "            remasking='low_confidence'\n",
    "        )\n",
    "\n",
    "        # Decode generated part\n",
    "        generated_text = tokenizer.batch_decode(\n",
    "            output_ids[:, input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "\n",
    "        # Store the result\n",
    "        results.append({\n",
    "            \"id\": item['id'],\n",
    "            \"question\": item['question'],\n",
    "            \"decomposition\": item.get('decomposition', ''),\n",
    "            f\"{num_shots}_shot_decomposition\": generated_text\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c970fb",
   "metadata": {},
   "source": [
    "##### $Save$ $the$ $results$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4423899",
   "metadata": {},
   "source": [
    "*Once the model processes the questions, its predictions for the zero-shot and few-shot experiments are saved in the corresponding folders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e861245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_json(results, folder, filename):\n",
    "\n",
    "    # Make sure the folder exists\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Construct the full path for the file\n",
    "    full_path = os.path.join(folder, filename)\n",
    "\n",
    "    # Save the file\n",
    "    with open(full_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Results have been saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6c313",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $HotpotQA$ $Dataset$ $Predictions$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff3fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "hotpot_dataset_path = \"/home/lathanasopoulou/capstone/search-in-ai/prompt-decomposition/HotpotQA/HotpotQA_dataset/hotpot_dataset.json\" # Define the path to the hotpot dataset\n",
    "\n",
    "# Load the dataset\n",
    "with open(hotpot_dataset_path, \"r\") as file:\n",
    "    hotpot_data = json.load(file)\n",
    "    print(f\"Loaded {len(hotpot_data)} questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62503c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First question details:\n",
      "ID: 5a8b57f25542995d1e6f1371\n",
      "Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "Answer: yes\n",
      "Supporting sentences: ['Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.', 'Edward Davis Wood Jr. (October 10, 1924 â€“ December 10, 1978) was an American filmmaker, actor, writer, producer, and director.', 'Aggregating the above we conclude that the answer is: yes']\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Decomposition: ['What nationality Scott Derrickson had?', 'What nationality Ed Wood had?', 'Was the nationality the same?']\n"
     ]
    }
   ],
   "source": [
    "# Display the first question\n",
    "print(\"First question details:\")\n",
    "print(\"ID:\", hotpot_data[0][\"id\"])\n",
    "print(\"Question:\", hotpot_data[0][\"question\"])\n",
    "print(\"Answer:\", hotpot_data[0][\"answer\"])\n",
    "print(\"Supporting sentences:\", hotpot_data[0][\"supporting_sentences\"])\n",
    "print(\"-\"*150)\n",
    "print(\"Decomposition:\", hotpot_data[0][\"decomposition\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaae847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the model for our file names\n",
    "model_file_name = \"LLaDA-8B-Base_Hotpot_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d0e9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 questions.\n"
     ]
    }
   ],
   "source": [
    "few_shot_hotpot_examples_path = \"/home/lathanasopoulou/capstone/search-in-ai/prompt-decomposition/HotpotQA/HotpotQA_dataset/hotpot_few_shot.json\"\n",
    "with open(few_shot_hotpot_examples_path, \"r\") as file:\n",
    "    shot_examples = json.load(file)\n",
    "    print(f\"Loaded {len(hotpot_data)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9b354",
   "metadata": {},
   "source": [
    "##### $Experiments$ $Run$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef9271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Zero-Shot Experiment\n",
      "Processing (Zero-Shot) ID: 5a8b57f25542995d1e6f1371\n",
      "Processing (Zero-Shot) ID: 5a8c7595554299585d9e36b6\n",
      "Processing (Zero-Shot) ID: 5a85ea095542994775f606a8\n",
      "Processing (Zero-Shot) ID: 5adbf0a255429947ff17385a\n",
      "Processing (Zero-Shot) ID: 5a8e3ea95542995a26add48d\n"
     ]
    }
   ],
   "source": [
    "# Run the zero-shot experiment\n",
    "print(\"\\nStarting Zero-Shot Experiment\")\n",
    "zero_shot_results = run_zero_shot_experiment(model, tokenizer, hotpot_data)\n",
    "save_results_to_json(zero_shot_results, zero_shot_folder, model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d793ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 3-Shot Experiment\n",
      "Processing (3-Shot) ID: 5a8b57f25542995d1e6f1371\n",
      "Processing (3-Shot) ID: 5a8c7595554299585d9e36b6\n",
      "Processing (3-Shot) ID: 5a85ea095542994775f606a8\n",
      "Processing (3-Shot) ID: 5adbf0a255429947ff17385a\n",
      "Processing (3-Shot) ID: 5a8e3ea95542995a26add48d\n",
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "# Run the few-shot experiment with 3 shots\n",
    "print(\"\\nStarting 3-Shot Experiment\")\n",
    "three_shot_results = run_few_shot_experiment(model, tokenizer, hotpot_data, shot_examples, num_shots=3)\n",
    "save_results_to_json(three_shot_results, few_shots_folder, f\"3shot_{model_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
