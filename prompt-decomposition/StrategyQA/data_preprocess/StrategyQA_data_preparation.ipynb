{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "105fff90",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $Libraries$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c88cda77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b3df5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Load$ $StrategyQA$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541ecb2",
   "metadata": {},
   "source": [
    "StrategyQA is a question answering benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. It contains `train` and `test` sets.\n",
    "\n",
    "* **Train/Test Set**:  The datasets have the same structure. The questions designed to require reasoning over multiple facts or concepts. Each example includes a question, a boolean answer (True/False), supporting facts, and related term descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b6c7ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_dataset = load_dataset(\"ChilleD/StrategyQA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "873dd51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy_dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41acd498",
   "metadata": {},
   "source": [
    "Each example contains:\n",
    "\n",
    "1. **`qid`**: Unique ID for the question.\n",
    "\n",
    "2. **`term`**: The topic or key phrase involved.\n",
    "\n",
    "3. **`description`**: Short explanation about the term.\n",
    "\n",
    "4. **`question`**: The actual question requiring reasoning.\n",
    "\n",
    "5. **`answer`**: The boolean answer (True/False).\n",
    "\n",
    "6. **`facts`**: Supporting facts used to arrive at the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44836394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['qid', 'term', 'description', 'question', 'answer', 'facts'],\n",
       "    num_rows: 1603\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1e38966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 4fd64bb6ce5b78ab20b6\n",
      "Term: Mixed martial arts\t | Description: full contact combat sport\n",
      "==================================================\n",
      "\n",
      "Question: Is Mixed martial arts totally original from Roman Colosseum games?\n",
      "\n",
      "Answer: False\n",
      "\n",
      "\n",
      "Supporting Facts\n",
      "--------------------------------------------------------------------------\n",
      "To answer the question, the model needs to find the following information:\n",
      "\n",
      "Mixed Martial arts in the UFC takes place in an enclosed structure called The Octagon. The Roman Colosseum games were fought in enclosed arenas where combatants would fight until the last man was standing. Mixed martial arts contests are stopped when one of the combatants is incapacitated. The Roman Colosseum was performed in front of crowds that numbered in the tens of thousands. Over 56,000 people attended UFC 193.\n"
     ]
    }
   ],
   "source": [
    "example_strategy = strategy_dataset[\"train\"][0]\n",
    "\n",
    "print(f\"ID: {example_strategy['qid']}\")\n",
    "print(f\"Term: {example_strategy['term']}\\t | Description: {example_strategy['description']}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nQuestion: {example_strategy['question']}\")\n",
    "print(f\"\\nAnswer: {example_strategy['answer']}\")\n",
    "\n",
    "print(\"\\n\\nSupporting Facts\")\n",
    "print(\"-\"*74)\n",
    "print(\"To answer the question, the model needs to find the following information:\")\n",
    "print(f\"\\n{example_strategy['facts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e91562",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Dataset$ $Creation$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9afd8",
   "metadata": {},
   "source": [
    "*First, we will create a few_shot_examples.json file by manually decomposing questions from the training set, using their supporting sentences as a guide. Next, we will build a separate evaluation dataset from the validation set, following the same manual decomposition process.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90ba4a",
   "metadata": {},
   "source": [
    "##### $Few$ $Shot$ $Examples$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de666d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7f77bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in strategy_dataset[\"train\"]:\n",
    "    example['id'] = example.pop('qid')\n",
    "\n",
    "    if len(few_shot_examples) < 5:\n",
    "        \n",
    "        few_shot_examples.append(example)\n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48afa900",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples[0][\"decomposition\"] = [\n",
    "    \t\t\t\"What is Mixed Martial Arts?\",\n",
    "                \"What were the Roman Colosseum games?\",\n",
    "                \"Are there similarities between MMA and Roman Colosseum games?\",\n",
    "                \"Are there differences between MMA and Roman Colosseum games?\",\n",
    "                \"Based on similarities and differences, is MMA totally original from Roman Colosseum games?\"]\n",
    "\n",
    "few_shot_examples[1][\"decomposition\"] = [\n",
    "                \"What defines vegan cuisine?\",\n",
    "                \"What are the main components of traditional Hawaiian cuisine?\",\n",
    "                \"Which traditional Hawaiian dishes or ingredients are naturally vegan or can be made vegan?\",\n",
    "                \"Are there common Hawaiian dishes that contain animal products making them unsuitable for vegans?\",\n",
    "                \"Based on this, is the overall cuisine of Hawaii suitable or adaptable for a vegan diet?\"]\n",
    "\n",
    "few_shot_examples[2][\"decomposition\"] = [\n",
    "    \t\t\t\"Can giant squid be captured in their natural habitat?\",\n",
    "                \"What equipment or gear is usually needed to capture a giant squid?\",\n",
    "                \"Is capturing giant squid without gear impossible?\"]\n",
    "\n",
    "few_shot_examples[3][\"decomposition\"] = [\n",
    "\t\t\t\t\"When did the Boxer Rebellion happen\",\n",
    "                \"When was the Royal Air Force (RAF) established?\",\n",
    "                \"Was the RAF active during the time of the Boxer Rebellion?\",\n",
    "                \"Based on the timelines, did the RAF participate in the Boxer Rebellion?\"]\n",
    "\n",
    "few_shot_examples[4][\"decomposition\"] = [\n",
    "                \"What is Solanum melongena commonly called in general English?\",\n",
    "                \"What are the common local names for Solanum melongena in Mumbai or India?\",\n",
    "                \"Is the term “eggplant” commonly used or understood by people in Mumbai?\",\n",
    "                \"Based on local language and culture, would “eggplant” be a usual reference in Mumbai?\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "caeac304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "filename = \"strategyqa_few_shot.json\"\n",
    "folder = \"../StrategyQA_dataset/\"\n",
    "\n",
    "# Construct the full path for the file\n",
    "full_path = os.path.join(folder, filename)\n",
    "\n",
    "# Save the file\n",
    "with open(full_path, 'w', encoding='utf-8') as f:\n",
    "\tjson.dump(few_shot_examples, f, ensure_ascii=False, indent=4)\n",
    "print(\"Results have been saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4f6f6",
   "metadata": {},
   "source": [
    "##### $Evaluation$ $Examples$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b14737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba3e9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in strategy_dataset[\"test\"]:\n",
    "    example['id'] = example.pop('qid')\n",
    "    if len(evaluation) < 5:\n",
    "        evaluation.append(example)\n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c7a64f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation[0][\"decomposition\"] = [\n",
    "    \t\t\t\"What was the name of the ship that recovered Apollo 13 astronauts?\",\n",
    "                \"Is that ship’s name connected to a World War II battle?\",\n",
    "                \"If yes, what was the World War II battle it was named after?\",\n",
    "                \"Based on this, was the ship named after a World War II battle?\"]\n",
    "\n",
    "evaluation[1][\"decomposition\"] = [\n",
    "                \"What is the tibia?\",\n",
    "                \"What role does the tibia play in playing hockey or sports in general?\",\n",
    "                \"Is having a functioning tibia essential for a player to participate effectively in hockey?\",\n",
    "                \"Can someone without a tibia still be part of a team that wins the Stanley Cup?\",\n",
    "                \"Based on this, is the tibia necessary to win the Stanley Cup?\"]\n",
    "\n",
    "evaluation[2][\"decomposition\"] = [\n",
    "    \t\t\t\"What does the Azerbaijani flag’s background look like?\",\n",
    "                \"IWho are the Powerpuff Girls, and what kind of artistic styles or backgrounds do they create?\",\n",
    "                \"Could the Powerpuff Girls (as characters or in their style) create or inspire a background similar to the Azerbaijani flag?\",\n",
    "                \"Are there any symbolic or design conflicts between the Powerpuff Girls style and the Azerbaijani flag’s elements?\",\n",
    "                \"Based on this, is it possible for the Powerpuff Girls to make the background to the Azerbaijani flag?\"]\n",
    "\n",
    "evaluation[3][\"decomposition\"] = [\n",
    "\t\t\t\t\"Are there youth groups named or labeled after 'Eagles' that focus on skills training?\",\n",
    "                \"Are there youth groups named or labeled after 'Young Bears' that focus on skills training?\"\n",
    "                \"What kinds of skills or activities do these groups typically teach?\",\n",
    "                \"Based on this, are both 'Eagles' and 'Young Bears' commonly used as names for skills-training youth groups?\"]\n",
    "\n",
    "evaluation[4][\"decomposition\"] = [\n",
    "                \"How physically fit are Olympic athletes generally?\",\n",
    "                \"What is the typical effort level for an Olympic athlete running a mile?\",\n",
    "                \"Does running a mile usually cause fatigue in athletes at this level?\",\n",
    "                \"Are there factors like pace, recovery, or event specialty that affect whether they’d be tired?\",\n",
    "                \"Based on this, would an Olympic athlete likely be tired out after running a mile?\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b0e90cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved!\n"
     ]
    }
   ],
   "source": [
    "filename = \"strategyqa_dataset.json\"\n",
    "folder = \"../StrategyQA_dataset/\"\n",
    "\n",
    "# Construct the full path for the file\n",
    "full_path = os.path.join(folder, filename)\n",
    "\n",
    "# Save the file\n",
    "with open(full_path, 'w', encoding='utf-8') as f:\n",
    "\tjson.dump(evaluation, f, ensure_ascii=False, indent=4)\n",
    "print(\"Results have been saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf572d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
